{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtRrxcPzDf5xr02i7hOSvN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazal735/DL_A3/blob/main/DL_A3(test).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90QL2a_Wlocp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "328e27fd-c695-48fe-ee98-0845439b9ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'transliteration_data.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f5d6a4eb465b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f5d6a4eb465b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'transliteration_data.txt'\u001b[0m  \u001b[0;31m# Replace with your data path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     \u001b[0meng_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindi_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;31m# Create vocabularies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f5d6a4eb465b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;31m# Function to load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transliteration_data.txt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Define constants\n",
        "HIDDEN_DIM = 128\n",
        "EMBEDDING_DIM = 64\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 20  # Adjust based on your data\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# Dataset class\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, eng_words, hindi_words, eng_vocab, hindi_vocab, max_length):\n",
        "        self.eng_words = eng_words\n",
        "        self.hindi_words = hindi_words\n",
        "        self.eng_vocab = eng_vocab\n",
        "        self.hindi_vocab = hindi_vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng_word = ['<SOS>'] + list(self.eng_words[idx]) + ['<EOS>']\n",
        "        hindi_word = ['<SOS>'] + list(self.hindi_words[idx]) + ['<EOS>']\n",
        "\n",
        "        # Pad sequences\n",
        "        eng_word = eng_word + ['<PAD>'] * (self.max_length - len(eng_word))\n",
        "        hindi_word = hindi_word + ['<PAD>'] * (self.max_length - len(hindi_word))\n",
        "\n",
        "        # Convert to indices\n",
        "        eng_indices = [self.eng_vocab.get(char, self.eng_vocab['<UNK>']) for char in eng_word]\n",
        "        hindi_indices = [self.hindi_vocab.get(char, self.hindi_vocab['<UNK>']) for char in hindi_word]\n",
        "\n",
        "        return {\n",
        "            'eng_indices': torch.tensor(eng_indices, dtype=torch.long),\n",
        "            'hindi_indices': torch.tensor(hindi_indices, dtype=torch.long),\n",
        "            'eng_length': len(self.eng_words[idx]) + 2,  # +2 for <SOS> and <EOS>\n",
        "            'hindi_length': len(self.hindi_words[idx]) + 2\n",
        "        }\n",
        "\n",
        "# Encoder with single layer RNN\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, dropout_p=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: [batch_size, seq_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # Use packed padded sequence if needed\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs shape: [batch_size, seq_len, hidden_dim]\n",
        "        # hidden shape: [1, batch_size, hidden_dim]\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "# Attention mechanism\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden shape: [1, batch_size, hidden_dim]\n",
        "        # encoder_outputs shape: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # Repeat hidden state for each position in sequence\n",
        "        hidden = hidden.transpose(0, 1).repeat(1, src_len, 1)\n",
        "        # hidden shape: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # Calculate energy\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        # energy shape: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        # attention shape: [batch_size, src_len]\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "# Decoder with attention\n",
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, attention, dropout_p=0.2):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim * 2 + embedding_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input shape: [batch_size, 1]\n",
        "        # hidden shape: [1, batch_size, hidden_dim]\n",
        "        # encoder_outputs shape: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        input = input.unsqueeze(1)  # Add sequence dimension: [batch_size, 1]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded shape: [batch_size, 1, embedding_dim]\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attention(hidden, encoder_outputs)\n",
        "        # attn_weights shape: [batch_size, src_len]\n",
        "\n",
        "        # Create weighted context vector\n",
        "        attn_weights = attn_weights.unsqueeze(1)\n",
        "        # attn_weights shape: [batch_size, 1, src_len]\n",
        "\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)\n",
        "        # context shape: [batch_size, 1, hidden_dim]\n",
        "\n",
        "        # Combine embedded input and context vector\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        # rnn_input shape: [batch_size, 1, embedding_dim + hidden_dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        # output shape: [batch_size, 1, hidden_dim]\n",
        "        # hidden shape: [1, batch_size, hidden_dim]\n",
        "\n",
        "        # Final prediction\n",
        "        embedded = embedded.squeeze(1)\n",
        "        output = output.squeeze(1)\n",
        "        context = context.squeeze(1)\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n",
        "        # prediction shape: [batch_size, output_dim]\n",
        "\n",
        "        return prediction, hidden, attn_weights\n",
        "\n",
        "# Seq2Seq with Attention\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src shape: [batch_size, src_len]\n",
        "        # trg shape: [batch_size, trg_len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Store attention weights for visualization\n",
        "        attention_weights = torch.zeros(batch_size, trg_len, src.shape[1]).to(self.device)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <SOS> token\n",
        "        decoder_input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Use previous hidden state to generate new state\n",
        "            decoder_output, hidden, attn_weights = self.decoder(decoder_input, hidden, encoder_outputs)\n",
        "\n",
        "            # Store prediction and attention weights\n",
        "            outputs[:, t, :] = decoder_output\n",
        "            attention_weights[:, t, :] = attn_weights.squeeze(1)\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # Get the highest predicted token\n",
        "            top1 = decoder_output.argmax(1)\n",
        "\n",
        "            # Use ground truth or predicted token as next input\n",
        "            decoder_input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "    def predict(self, src, max_length, sos_idx, eos_idx):\n",
        "        # Set the model to evaluation mode\n",
        "        self.eval()\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        # Store predictions and attention maps\n",
        "        predictions = torch.zeros(batch_size, max_length, dtype=torch.long).to(self.device)\n",
        "        attention_maps = torch.zeros(batch_size, max_length, src.shape[1]).to(self.device)\n",
        "\n",
        "        # Encode input sequence\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # First decoder input is <SOS> token\n",
        "        decoder_input = torch.tensor([sos_idx] * batch_size).to(self.device)\n",
        "\n",
        "        # Initialize list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(1, max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, hidden, attn_weights = self.decoder(decoder_input, hidden, encoder_outputs)\n",
        "\n",
        "            # Get most likely word index\n",
        "            top1 = decoder_output.argmax(1)\n",
        "\n",
        "            # Save attention weights for visualization\n",
        "            attention_maps[:, t, :] = attn_weights.squeeze(1)\n",
        "\n",
        "            # Save prediction\n",
        "            predictions[:, t] = top1\n",
        "\n",
        "            # Next input is current prediction\n",
        "            decoder_input = top1\n",
        "\n",
        "            # Store outputs for each batch\n",
        "            outputs.append(top1.unsqueeze(1))\n",
        "\n",
        "            # Stop if all sequences have reached EOS\n",
        "            if all([(top1[i] == eos_idx) for i in range(batch_size)]):\n",
        "                break\n",
        "\n",
        "        # Return predictions and attention maps\n",
        "        return predictions, attention_maps\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train(model, dataloader, optimizer, criterion, clip, device, teacher_forcing_ratio):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        src = batch['eng_indices'].to(device)\n",
        "        trg = batch['hindi_indices'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output, _ = model(src, trg, teacher_forcing_ratio)\n",
        "\n",
        "        # Calculate loss (ignore first token which is <SOS>)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src = batch['eng_indices'].to(device)\n",
        "            trg = batch['hindi_indices'].to(device)\n",
        "\n",
        "            # Forward pass (no teacher forcing)\n",
        "            output, _ = model(src, trg, 0)\n",
        "\n",
        "            # Calculate loss\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def calculate_accuracy(model, dataloader, hindi_vocab, device, max_length):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Create reverse vocabulary for decoding predictions\n",
        "    rev_hindi_vocab = {idx: char for char, idx in hindi_vocab.items()}\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src = batch['eng_indices'].to(device)\n",
        "            trg = batch['hindi_indices'].to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            pred, _ = model.predict(src, max_length, hindi_vocab['<SOS>'], hindi_vocab['<EOS>'])\n",
        "\n",
        "            # Remove <SOS> token from target\n",
        "            trg = trg[:, 1:]\n",
        "\n",
        "            # Process each sequence in the batch\n",
        "            for i in range(trg.size(0)):\n",
        "                # Convert predictions to characters\n",
        "                pred_seq = []\n",
        "                for j in range(1, pred.size(1)):  # Start from 1 to skip <SOS>\n",
        "                    if pred[i, j].item() == hindi_vocab['<EOS>']:\n",
        "                        break\n",
        "                    pred_seq.append(rev_hindi_vocab[pred[i, j].item()])\n",
        "\n",
        "                # Convert target to characters\n",
        "                trg_seq = []\n",
        "                for j in range(trg.size(1)):\n",
        "                    if trg[i, j].item() == hindi_vocab['<EOS>']:\n",
        "                        break\n",
        "                    if trg[i, j].item() == hindi_vocab['<PAD>']:\n",
        "                        continue\n",
        "                    trg_seq.append(rev_hindi_vocab[trg[i, j].item()])\n",
        "\n",
        "                # Store prediction\n",
        "                predictions.append({\n",
        "                    'predicted': ''.join(pred_seq),\n",
        "                    'target': ''.join(trg_seq)\n",
        "                })\n",
        "\n",
        "                # Check if prediction matches target\n",
        "                if ''.join(pred_seq) == ''.join(trg_seq):\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, predictions\n",
        "\n",
        "def save_predictions(predictions, filename):\n",
        "    df = pd.DataFrame(predictions)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Predictions saved to {filename}\")\n",
        "\n",
        "def plot_attention_heatmap(attention, src_text, trg_text, ax=None):\n",
        "    \"\"\"Plot attention heatmap for a single example\"\"\"\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Trim to actual sequence length (remove padding)\n",
        "    src_len = len([c for c in src_text if c != '<PAD>'])\n",
        "    trg_len = len([c for c in trg_text if c != '<PAD>'])\n",
        "\n",
        "    attn_display = attention[:trg_len, :src_len]\n",
        "\n",
        "    # Display heatmap\n",
        "    sns.heatmap(attn_display, cmap='viridis', ax=ax)\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xticklabels(src_text[:src_len], rotation=90)\n",
        "    ax.set_yticklabels(trg_text[:trg_len])\n",
        "\n",
        "    ax.set_xlabel('Source')\n",
        "    ax.set_ylabel('Target')\n",
        "\n",
        "    return ax\n",
        "\n",
        "def plot_attention_grid(model, dataset, test_indices, device, eng_vocab, hindi_vocab, max_length=20, num_examples=9):\n",
        "    \"\"\"Create a grid of attention heatmaps\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create reverse vocabularies\n",
        "    rev_eng_vocab = {idx: char for char, idx in eng_vocab.items()}\n",
        "    rev_hindi_vocab = {idx: char for char, idx in hindi_vocab.items()}\n",
        "\n",
        "    # Set up the plot\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(test_indices[:num_examples]):\n",
        "            sample = dataset[idx]\n",
        "            src = sample['eng_indices'].unsqueeze(0).to(device)\n",
        "\n",
        "            # Get predictions and attention weights\n",
        "            preds, attention_weights = model.predict(src, max_length, hindi_vocab['<SOS>'], hindi_vocab['<EOS>'])\n",
        "\n",
        "            # Convert indices to characters\n",
        "            src_chars = [rev_eng_vocab[idx.item()] for idx in sample['eng_indices']]\n",
        "\n",
        "            # Get predicted chars\n",
        "            pred_chars = ['<SOS>']\n",
        "            for j in range(1, preds.size(1)):\n",
        "                if preds[0, j].item() == hindi_vocab['<EOS>']:\n",
        "                    pred_chars.append('<EOS>')\n",
        "                    break\n",
        "                pred_chars.append(rev_hindi_vocab[preds[0, j].item()])\n",
        "\n",
        "            # Plot attention heatmap\n",
        "            plot_attention_heatmap(attention_weights[0].cpu().numpy(),\n",
        "                                  src_chars,\n",
        "                                  pred_chars,\n",
        "                                  ax=axes[i])\n",
        "\n",
        "            axes[i].set_title(f'Input: {\"\".join([c for c in src_chars if c not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]])} → Output: {\"\".join([c for c in pred_chars if c not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]])}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('attention_heatmaps.png')\n",
        "    plt.close()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Function to load and prepare data\n",
        "def load_data(data_path):\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    eng_words = []\n",
        "    hindi_words = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            eng_words.append(parts[0].lower())\n",
        "            hindi_words.append(parts[1])\n",
        "\n",
        "    return eng_words, hindi_words\n",
        "\n",
        "def create_vocabularies(eng_words, hindi_words):\n",
        "    eng_chars = set()\n",
        "    hindi_chars = set()\n",
        "\n",
        "    for word in eng_words:\n",
        "        eng_chars.update(list(word))\n",
        "\n",
        "    for word in hindi_words:\n",
        "        hindi_chars.update(list(word))\n",
        "\n",
        "    # Create vocabularies\n",
        "    eng_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "    for i, char in enumerate(sorted(eng_chars)):\n",
        "        eng_vocab[char] = i + 4\n",
        "\n",
        "    hindi_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "    for i, char in enumerate(sorted(hindi_chars)):\n",
        "        hindi_vocab[char] = i + 4\n",
        "\n",
        "    return eng_vocab, hindi_vocab\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    data_path = 'transliteration_data.txt'  # Replace with your data path\n",
        "    eng_words, hindi_words = load_data(data_path)\n",
        "\n",
        "    # Create vocabularies\n",
        "    eng_vocab, hindi_vocab = create_vocabularies(eng_words, hindi_words)\n",
        "\n",
        "    # Split data\n",
        "    eng_train, eng_temp, hindi_train, hindi_temp = train_test_split(\n",
        "        eng_words, hindi_words, test_size=0.3, random_state=42\n",
        "    )\n",
        "    eng_val, eng_test, hindi_val, hindi_test = train_test_split(\n",
        "        eng_temp, hindi_temp, test_size=0.5, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TransliterationDataset(eng_train, hindi_train, eng_vocab, hindi_vocab, MAX_LENGTH)\n",
        "    val_dataset = TransliterationDataset(eng_val, hindi_val, eng_vocab, hindi_vocab, MAX_LENGTH)\n",
        "    test_dataset = TransliterationDataset(eng_test, hindi_test, eng_vocab, hindi_vocab, MAX_LENGTH)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Model dimensions\n",
        "    input_dim = len(eng_vocab)\n",
        "    output_dim = len(hindi_vocab)\n",
        "\n",
        "    # Initialize models\n",
        "    encoder = Encoder(input_dim, EMBEDDING_DIM, HIDDEN_DIM, DROPOUT)\n",
        "    attention = Attention(HIDDEN_DIM)\n",
        "    decoder = AttentionDecoder(output_dim, EMBEDDING_DIM, HIDDEN_DIM, attention, DROPOUT)\n",
        "    model = Seq2SeqAttention(encoder, decoder, device).to(device)\n",
        "\n",
        "    # Print model architecture\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "    # Initialize loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=hindi_vocab['<PAD>'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Training loop\n",
        "    best_valid_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train(model, train_dataloader, optimizer, criterion, 1.0, device, TEACHER_FORCING_RATIO)\n",
        "        valid_loss = evaluate(model, val_dataloader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(valid_loss)\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_mins = int((end_time - start_time) / 60)\n",
        "        epoch_secs = int((end_time - start_time) % 60)\n",
        "\n",
        "        # Save best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'best-seq2seq-attention-model.pt')\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "        print(f'\\tValid Loss: {valid_loss:.3f}')\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.savefig('attention_training_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best-seq2seq-attention-model.pt'))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss = evaluate(model, test_dataloader, criterion, device)\n",
        "    print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "    # Calculate accuracy on test set\n",
        "    test_accuracy, test_predictions = calculate_accuracy(model, test_dataloader, hindi_vocab, device, MAX_LENGTH)\n",
        "    print(f'Test Accuracy: {test_accuracy:.3f}')\n",
        "\n",
        "    # Save predictions\n",
        "    save_predictions(test_predictions, 'predictions_attention.csv')\n",
        "\n",
        "    # Create attention heatmaps for sample test examples\n",
        "    test_indices = random.sample(range(len(test_dataset)), 9)\n",
        "    attention_fig = plot_attention_grid(model, test_dataset, test_indices, device, eng_vocab, hindi_vocab)\n",
        "\n",
        "    return model, test_dataset, test_indices, eng_vocab, hindi_vocab, device\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OARFhVQ7li0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}