{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazal735/DL_A3/blob/main/DL_A3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW9jUy9HnQqZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, RNN, LSTM, GRU, SimpleRNN\n",
        "\n",
        "\n",
        "class TransliterationSeq2Seq(Model):\n",
        "    def __init__(self,\n",
        "                 input_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 embedding_dim,\n",
        "                 encoder_units,\n",
        "                 decoder_units,\n",
        "                 cell_type='lstm',\n",
        "                 encoder_layers=1,\n",
        "                 decoder_layers=1):\n",
        "        \"\"\"\n",
        "        Initialize the Seq2Seq model for transliteration.\n",
        "\n",
        "        Args:\n",
        "            input_vocab_size: Size of the input language vocabulary\n",
        "            target_vocab_size: Size of the target language vocabulary\n",
        "            embedding_dim: Dimension of character embeddings\n",
        "            encoder_units: Number of units in encoder cell\n",
        "            decoder_units: Number of units in decoder cell\n",
        "            cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "            encoder_layers: Number of layers in the encoder\n",
        "            decoder_layers: Number of layers in the decoder\n",
        "        \"\"\"\n",
        "        super(TransliterationSeq2Seq, self).__init__()\n",
        "\n",
        "        # Store model parameters\n",
        "        self.encoder_units = encoder_units\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "        # Create embeddings for input characters\n",
        "        self.embedding = Embedding(input_vocab_size, embedding_dim)\n",
        "\n",
        "        # Setup the cell type based on user preference\n",
        "        self.cell_type = cell_type.lower()\n",
        "        if self.cell_type == 'lstm':\n",
        "            rnn_cell = LSTM\n",
        "        elif self.cell_type == 'gru':\n",
        "            rnn_cell = GRU\n",
        "        elif self.cell_type == 'rnn':\n",
        "            rnn_cell = SimpleRNN\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        # Create encoder layers\n",
        "        self.encoder_layers = []\n",
        "        for i in range(encoder_layers):\n",
        "            input_size = embedding_dim if i == 0 else encoder_units\n",
        "            return_sequences = (i < encoder_layers - 1)\n",
        "            self.encoder_layers.append(\n",
        "                rnn_cell(encoder_units,\n",
        "                         return_sequences=return_sequences,\n",
        "                         return_state=True,\n",
        "                         name=f'encoder_layer_{i+1}')\n",
        "            )\n",
        "\n",
        "        # Create decoder layers\n",
        "        self.decoder_layers = []\n",
        "        for i in range(decoder_layers):\n",
        "            input_size = embedding_dim if i == 0 else decoder_units\n",
        "            return_sequences = True\n",
        "            self.decoder_layers.append(\n",
        "                rnn_cell(decoder_units,\n",
        "                         return_sequences=return_sequences,\n",
        "                         return_state=True,\n",
        "                         name=f'decoder_layer_{i+1}')\n",
        "            )\n",
        "\n",
        "        # Output layer to convert decoder output to character probabilities\n",
        "        self.fc = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "        if encoder_units != decoder_units:\n",
        "            if self.cell_type == 'lstm':\n",
        "                self.state_h_adapter = tf.keras.layers.Dense(decoder_units, name='state_h_adapter')\n",
        "                self.state_c_adapter = tf.keras.layers.Dense(decoder_units, name='state_c_adapter')\n",
        "            else:\n",
        "                self.state_adapter = tf.keras.layers.Dense(decoder_units, name='state_adapter')\n",
        "\n",
        "    def encode(self, input_seq):\n",
        "        \"\"\"\n",
        "        Encode the input sequence.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "\n",
        "        Returns:\n",
        "            encoder_outputs: Outputs from the encoder\n",
        "            encoder_states: Final states from the encoder (to initialize decoder)\n",
        "        \"\"\"\n",
        "        # Embed input sequence\n",
        "        x = self.embedding(input_seq)\n",
        "\n",
        "        # Process through encoder layers\n",
        "        encoder_states = []\n",
        "        for i, encoder_layer in enumerate(self.encoder_layers):\n",
        "            if i == 0:\n",
        "                outputs = x\n",
        "\n",
        "            if self.cell_type == 'lstm':\n",
        "                outputs, state_h, state_c = encoder_layer(outputs)\n",
        "                encoder_states.extend([state_h, state_c])\n",
        "            else:  # RNN or GRU\n",
        "                outputs, state = encoder_layer(outputs)\n",
        "                encoder_states.append(state)\n",
        "\n",
        "        return outputs, encoder_states\n",
        "\n",
        "    def adapt_encoder_states_for_decoder(self, encoder_states):\n",
        "        \"\"\"Adapt encoder states to be compatible with decoder dimensions\"\"\"\n",
        "        decoder_states = []\n",
        "\n",
        "        for state in encoder_states:\n",
        "            if self.cell_type == 'lstm':\n",
        "                state_h, state_c = state\n",
        "                if hasattr(self, 'state_h_adapter'):\n",
        "                    adapted_h = self.state_h_adapter(state_h)\n",
        "                    adapted_c = self.state_c_adapter(state_c)\n",
        "                    decoder_states.append((adapted_h, adapted_c))\n",
        "                else:\n",
        "                    decoder_states.append((state_h, state_c))\n",
        "            else:\n",
        "                if hasattr(self, 'state_adapter'):\n",
        "                    adapted_state = self.state_adapter(state)\n",
        "                    decoder_states.append(adapted_state)\n",
        "                else:\n",
        "                    decoder_states.append(state)\n",
        "\n",
        "        return decoder_states\n",
        "\n",
        "    def decode_step(self, x, states):\n",
        "        \"\"\"\n",
        "        Perform one decoding step.\n",
        "\n",
        "        Args:\n",
        "            x: Input character tensor of shape (batch_size, 1)\n",
        "            states: Previous states from the decoder\n",
        "\n",
        "        Returns:\n",
        "            output: Output probabilities\n",
        "            new_states: Updated states\n",
        "        \"\"\"\n",
        "        # Embed input character\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Process through decoder layers\n",
        "        all_new_states = []\n",
        "        for i, decoder_layer in enumerate(self.decoder_layers):\n",
        "            if i == 0:\n",
        "                layer_input = x\n",
        "            else:\n",
        "                layer_input = outputs\n",
        "\n",
        "            # Extract the states for this layer\n",
        "            if self.cell_type == 'lstm':\n",
        "                layer_states = [states[i*2], states[i*2+1]]\n",
        "            else:  # RNN or GRU\n",
        "                layer_states = [states[i]]\n",
        "\n",
        "            # Process through the layer\n",
        "            if self.cell_type == 'lstm':\n",
        "                outputs, state_h, state_c = decoder_layer(layer_input, initial_state=layer_states)\n",
        "                all_new_states.extend([state_h, state_c])\n",
        "            else:  # RNN or GRU\n",
        "                outputs, state = decoder_layer(layer_input, initial_state=layer_states)\n",
        "                all_new_states.append(state)\n",
        "\n",
        "        # Generate output probabilities\n",
        "        output = self.fc(outputs)\n",
        "\n",
        "        return output, all_new_states\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            inputs: Tuple of (input_seq, target_seq)\n",
        "            training: Whether the model is in training mode\n",
        "\n",
        "        Returns:\n",
        "            outputs: Sequence of output probabilities\n",
        "        \"\"\"\n",
        "        input_seq, target_seq = inputs\n",
        "\n",
        "        # Encode the input sequence\n",
        "        encoder_outputs, encoder_states = self.encode(input_seq)\n",
        "\n",
        "        # Initialize decoder states with encoder states\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Teacher forcing: feeding the target as the next input\n",
        "        decoder_inputs = target_seq[:, :-1]  # exclude last character\n",
        "\n",
        "        # Initialize list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        # Process each character in the target sequence\n",
        "        for t in range(decoder_inputs.shape[1]):\n",
        "            # Extract input for this timestep\n",
        "            decoder_input = decoder_inputs[:, t:t+1]\n",
        "\n",
        "            # Perform one decoding step\n",
        "            output, decoder_states = self.decode_step(decoder_input, decoder_states)\n",
        "\n",
        "            # Store output\n",
        "            outputs.append(output)\n",
        "\n",
        "        # Concatenate outputs along time axis\n",
        "        return tf.concat(outputs, axis=1)\n",
        "\n",
        "    def predict(self, input_seq, max_length=100, start_token=1, end_token=2):\n",
        "        \"\"\"\n",
        "        Generate a transliteration for an input sequence.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "            max_length: Maximum length of the output sequence\n",
        "            start_token: ID of the start token\n",
        "            end_token: ID of the end token\n",
        "\n",
        "        Returns:\n",
        "            outputs: Generated output sequence\n",
        "        \"\"\"\n",
        "        # Encode the input sequence\n",
        "        _, encoder_states = self.encode(input_seq)\n",
        "\n",
        "        # Initialize decoder states with encoder states\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Start with the start token\n",
        "        batch_size = input_seq.shape[0]\n",
        "        decoder_input = tf.ones((batch_size, 1), dtype=tf.int32) * start_token\n",
        "\n",
        "        # Initialize list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        # Generate characters until max_length or end token\n",
        "        for t in range(max_length):\n",
        "            # Perform one decoding step\n",
        "            output, decoder_states = self.decode_step(decoder_input, decoder_states)\n",
        "\n",
        "            # Get the most likely character\n",
        "            predicted_id = tf.argmax(output, axis=-1)\n",
        "\n",
        "            # Store output\n",
        "            outputs.append(predicted_id)\n",
        "\n",
        "            # Break if end token is predicted\n",
        "            if predicted_id == end_token:\n",
        "                break\n",
        "\n",
        "            # Use predicted ID as next input\n",
        "            decoder_input = predicted_id\n",
        "\n",
        "        # Concatenate outputs along time axis\n",
        "        return tf.concat(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "def create_model(input_vocab_size=1000,\n",
        "                target_vocab_size=1000,\n",
        "                embedding_dim=256,\n",
        "                encoder_units=512,\n",
        "                decoder_units=512,\n",
        "                cell_type='lstm',\n",
        "                encoder_layers=1,\n",
        "                decoder_layers=1):\n",
        "    \"\"\"\n",
        "    Create and compile a transliteration model.\n",
        "\n",
        "    Args:\n",
        "        input_vocab_size: Size of the input vocabulary\n",
        "        target_vocab_size: Size of the target vocabulary\n",
        "        embedding_dim: Dimension of character embeddings\n",
        "        encoder_units: Number of units in encoder cell\n",
        "        decoder_units: Number of units in decoder cell\n",
        "        cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "        encoder_layers: Number of layers in the encoder\n",
        "        decoder_layers: Number of layers in the decoder\n",
        "\n",
        "    Returns:\n",
        "        model: Compiled transliteration model\n",
        "    \"\"\"\n",
        "    # Create inputs\n",
        "    input_seq = Input(shape=(None,), name='input_sequence')\n",
        "    target_seq = Input(shape=(None,), name='target_sequence')\n",
        "\n",
        "    # Create model\n",
        "    model = TransliterationSeq2Seq(\n",
        "        input_vocab_size=input_vocab_size,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        encoder_units=encoder_units,\n",
        "        decoder_units=decoder_units,\n",
        "        cell_type=cell_type,\n",
        "        encoder_layers=encoder_layers,\n",
        "        decoder_layers=decoder_layers\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Training example\n",
        "def train_model(model, input_sequences, target_sequences, epochs=10, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train the transliteration model.\n",
        "\n",
        "    Args:\n",
        "        model: Compiled transliteration model\n",
        "        input_sequences: Input sequences (Latin script)\n",
        "        target_sequences: Target sequences (Devanagari script)\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Batch size for training\n",
        "\n",
        "    Returns:\n",
        "        history: Training history\n",
        "    \"\"\"\n",
        "    # Prepare target sequences for training (shifted by 1)\n",
        "    shifted_target_sequences = target_sequences[:, 1:]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [input_sequences, target_sequences],\n",
        "        shifted_target_sequences,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h19j_cRLtYZe",
        "outputId": "accd6fe0-e8fc-4e70-e595-ee1637be2668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: wandb 0.19.11\n",
            "Uninstalling wandb-0.19.11:\n",
            "  Successfully uninstalled wandb-0.19.11\n",
            "\u001b[33mWARNING: Skipping wandb-sdk as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping wandb-core as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip uninstall -y wandb\n",
        "!pip uninstall -y wandb-sdk\n",
        "!pip uninstall -y wandb-core\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSyFN5X2wEL6",
        "outputId": "bd0c16e8-fee1-4cfe-c22d-895b4a22cccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Using cached wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Using cached wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
            "Installing collected packages: wandb\n",
            "Successfully installed wandb-0.19.11\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, RNN, LSTM, GRU, SimpleRNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRh80Y_cDGDK",
        "outputId": "af6bfe15-ecb5-492e-e659-d485edd74cef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and preprocessing\n",
        "\n",
        "\n",
        "def load_dakshina_data(language='ur', data_dir='/content/drive/MyDrive/dakshina_dataset_v1.0/'):\n",
        "\n",
        "    data_path = os.path.join(data_dir, language, 'lexicons')\n",
        "\n",
        "    train_file = os.path.join(data_path, f'{language}.translit.sampled.train.tsv')\n",
        "    dev_file = os.path.join(data_path, f'{language}.translit.sampled.dev.tsv')\n",
        "    test_file = os.path.join(data_path, f'{language}.translit.sampled.test.tsv')\n",
        "\n",
        "    train_data = pd.read_csv(train_file, sep='\\t', header=None, names=['native', 'latin','n'])\n",
        "    dev_data = pd.read_csv(dev_file, sep='\\t', header=None, names=['native', 'latin','n'])\n",
        "    test_data = pd.read_csv(test_file, sep='\\t', header=None, names=['native', 'latin','n'])\n",
        "\n",
        "\n",
        "    print(f\"Loaded {len(train_data)} training, {len(dev_data)} dev, and {len(test_data)} test examples.\")\n",
        "\n",
        "    return train_data, dev_data, test_data"
      ],
      "metadata": {
        "id": "lYAp9zQkDMFa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def preprocess_data(train_data, dev_data, test_data):\n",
        "    \"\"\"\n",
        "    Preprocess the data: tokenize, create vocabulary, and convert to sequences.\n",
        "\n",
        "    Args:\n",
        "        train_data, dev_data, test_data: DataFrames with native and latin script pairs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing preprocessed data and tokenizers\n",
        "    \"\"\"\n",
        "\n",
        "    print(train_data.head())\n",
        "\n",
        "    # Clean the 'latin' column: remove NaNs and convert all to strings\n",
        "    train_data['latin'] = train_data['latin'].astype(str).fillna('')\n",
        "\n",
        "    # Or, to be safer and remove completely invalid rows:\n",
        "    train_data = train_data.dropna(subset=['latin'])\n",
        "    train_data['latin'] = train_data['latin'].astype(str)\n",
        "\n",
        "\n",
        "    # Create character-level tokenizers\n",
        "    latin_tokenizer = Tokenizer(char_level=True)\n",
        "    native_tokenizer = Tokenizer(char_level=True)\n",
        "\n",
        "    # Fit tokenizers on training data\n",
        "    latin_tokenizer.fit_on_texts((train_data['latin']).tolist())\n",
        "    native_tokenizer.fit_on_texts((train_data['native']).tolist())\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    latin_vocab_size = len(latin_tokenizer.word_index) + 1  # +1 for padding\n",
        "    native_vocab_size = len(native_tokenizer.word_index) + 1  # +1 for padding\n",
        "\n",
        "    # Convert text to sequences\n",
        "    train_latin_seq = latin_tokenizer.texts_to_sequences(list(train_data['latin']))\n",
        "    train_native_seq = native_tokenizer.texts_to_sequences(list(train_data['native']))\n",
        "\n",
        "    dev_latin_seq = latin_tokenizer.texts_to_sequences(list(dev_data['latin']))\n",
        "    dev_native_seq = native_tokenizer.texts_to_sequences(list(dev_data['native']))\n",
        "\n",
        "    test_latin_seq = latin_tokenizer.texts_to_sequences(list(test_data['latin']))\n",
        "    test_native_seq = native_tokenizer.texts_to_sequences(list(test_data['native']))\n",
        "\n",
        "    # Find maximum sequence lengths\n",
        "    max_latin_len = max(max(len(seq) for seq in train_latin_seq),\n",
        "                        max(len(seq) for seq in dev_latin_seq),\n",
        "                        max(len(seq) for seq in test_latin_seq))\n",
        "\n",
        "    max_native_len = max(max(len(seq) for seq in train_native_seq),\n",
        "                         max(len(seq) for seq in dev_native_seq),\n",
        "                         max(len(seq) for seq in test_native_seq))\n",
        "\n",
        "    # Add 2 for start and end tokens\n",
        "    max_latin_len += 2\n",
        "    max_native_len += 2\n",
        "\n",
        "    # Add start (<s>) and end (</s>) tokens to target sequences\n",
        "    start_token = native_vocab_size  # Use vocab_size as start token\n",
        "    end_token = native_vocab_size + 1  # Use vocab_size+1 as end token\n",
        "    native_vocab_size += 2  # Increase vocab size for start and end tokens\n",
        "\n",
        "    # Add start and end tokens to native sequences\n",
        "    def add_start_end(sequences, max_len, start_token, end_token):\n",
        "        new_sequences = []\n",
        "        for seq in sequences:\n",
        "            new_seq = [start_token] + seq + [end_token]\n",
        "            new_sequences.append(new_seq)\n",
        "        return pad_sequences(new_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Pad sequences\n",
        "    train_latin_padded = pad_sequences(train_latin_seq, maxlen=max_latin_len, padding='post')\n",
        "    train_native_padded = add_start_end(train_native_seq, max_native_len, start_token, end_token)\n",
        "\n",
        "    dev_latin_padded = pad_sequences(dev_latin_seq, maxlen=max_latin_len, padding='post')\n",
        "    dev_native_padded = add_start_end(dev_native_seq, max_native_len, start_token, end_token)\n",
        "\n",
        "    test_latin_padded = pad_sequences(test_latin_seq, maxlen=max_latin_len, padding='post')\n",
        "    test_native_padded = add_start_end(test_native_seq, max_native_len, start_token, end_token)\n",
        "\n",
        "    # Target for teacher forcing (shifted right by one position)\n",
        "    train_native_target = train_native_padded[:, 1:]  # Remove start token\n",
        "    dev_native_target = dev_native_padded[:, 1:]  # Remove start token\n",
        "\n",
        "    return {\n",
        "        'latin_tokenizer': latin_tokenizer,\n",
        "        'native_tokenizer': native_tokenizer,\n",
        "        'latin_vocab_size': latin_vocab_size,\n",
        "        'native_vocab_size': native_vocab_size,\n",
        "        'max_latin_len': max_latin_len,\n",
        "        'max_native_len': max_native_len,\n",
        "        'start_token': start_token,\n",
        "        'end_token': end_token,\n",
        "        'train_latin': train_latin_padded,\n",
        "        'train_native': train_native_padded,\n",
        "        'train_native_target': train_native_target,\n",
        "        'dev_latin': dev_latin_padded,\n",
        "        'dev_native': dev_native_padded,\n",
        "        'dev_native_target': dev_native_target,\n",
        "        'test_latin': test_latin_padded,\n",
        "        'test_native': test_native_padded\n",
        "    }\n"
      ],
      "metadata": {
        "id": "nbQLy9sVDeGz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Enhanced Seq2Seq model with dropout and beam search\n",
        "class TransliterationSeq2Seq(Model):\n",
        "    def __init__(self,\n",
        "                 input_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 embedding_dim,\n",
        "                 decoder_hidden_dim,\n",
        "                 encoder_units,\n",
        "                 decoder_units,\n",
        "                 cell_type='lstm',\n",
        "                 encoder_layers=1,\n",
        "                 decoder_layers=1,\n",
        "                 dropout_rate=0.0,\n",
        "\n",
        "\n",
        "                 recurrent_dropout_rate=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the Seq2Seq model for transliteration with dropout support.\n",
        "\n",
        "        Args:\n",
        "            input_vocab_size: Size of the input language vocabulary\n",
        "            target_vocab_size: Size of the target language vocabulary\n",
        "            embedding_dim: Dimension of character embeddings\n",
        "            encoder_units: Number of units in encoder cell\n",
        "            decoder_units: Number of units in decoder cell\n",
        "            cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "            encoder_layers: Number of layers in the encoder\n",
        "            decoder_layers: Number of layers in the decoder\n",
        "            dropout_rate: Dropout rate after RNN layers\n",
        "            recurrent_dropout_rate: Dropout rate inside RNN cells\n",
        "        \"\"\"\n",
        "        super(TransliterationSeq2Seq, self).__init__()\n",
        "\n",
        "        # Store model parameters\n",
        "        self.encoder_units = encoder_units\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.target_embedding = tf.keras.layers.Embedding(input_dim=self.target_vocab_size,\n",
        "                                                          output_dim=self.embedding_dim )\n",
        "\n",
        "        self.embedding_projection = tf.keras.layers.Dense(decoder_hidden_dim)\n",
        "\n",
        "        # Create embeddings for input and target characters\n",
        "        self.input_embedding = Embedding(input_vocab_size, embedding_dim)\n",
        "        self.target_embedding = Embedding(target_vocab_size, embedding_dim)\n",
        "\n",
        "        # Setup the cell type based on user preference\n",
        "        self.cell_type = cell_type.lower()\n",
        "        rnn_kwargs = {'return_state': True}\n",
        "\n",
        "        # Add recurrent dropout if specified\n",
        "        if recurrent_dropout_rate > 0:\n",
        "            rnn_kwargs['recurrent_dropout'] = recurrent_dropout_rate\n",
        "\n",
        "        if self.cell_type == 'lstm':\n",
        "            rnn_cell = LSTM\n",
        "        elif self.cell_type == 'gru':\n",
        "            rnn_cell = GRU\n",
        "        elif self.cell_type == 'rnn':\n",
        "            rnn_cell = SimpleRNN\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        # Create encoder layers\n",
        "        self.encoder_layers = []\n",
        "        for i in range(encoder_layers):\n",
        "            return_sequences = (i < encoder_layers - 1)\n",
        "            layer_kwargs = rnn_kwargs.copy()\n",
        "            layer_kwargs['return_sequences'] = return_sequences\n",
        "            layer_kwargs['name'] = f'encoder_layer_{i+1}'\n",
        "\n",
        "            self.encoder_layers.append(rnn_cell(encoder_units, **layer_kwargs))\n",
        "\n",
        "            # Add dropout between layers if specified\n",
        "            if dropout_rate > 0 and i < encoder_layers - 1:\n",
        "                self.encoder_layers.append(Dropout(dropout_rate))\n",
        "\n",
        "        # Create decoder layers\n",
        "        self.decoder_layers = []\n",
        "        for i in range(decoder_layers):\n",
        "            return_sequences = True\n",
        "            layer_kwargs = rnn_kwargs.copy()\n",
        "            layer_kwargs['return_sequences'] = return_sequences\n",
        "            layer_kwargs['name'] = f'decoder_layer_{i+1}'\n",
        "\n",
        "            self.decoder_layers.append(rnn_cell(decoder_units, **layer_kwargs))\n",
        "\n",
        "            # Add dropout between layers if specified\n",
        "            if dropout_rate > 0 and i < decoder_layers - 1:\n",
        "                self.decoder_layers.append(Dropout(dropout_rate))\n",
        "\n",
        "        # Final dropout before output layer\n",
        "        if dropout_rate > 0:\n",
        "            self.final_dropout = Dropout(dropout_rate)\n",
        "\n",
        "        # Output layer to convert decoder output to character probabilities\n",
        "        self.fc = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def encode(self, input_seq):\n",
        "        \"\"\"\n",
        "        Encode the input sequence.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "\n",
        "        Returns:\n",
        "            encoder_outputs: Outputs from the encoder\n",
        "            encoder_states: Final states from the encoder (to initialize decoder)\n",
        "        \"\"\"\n",
        "        # Embed input sequence\n",
        "        x = self.input_embedding(input_seq)\n",
        "\n",
        "        # Process through encoder layers\n",
        "        encoder_states = []\n",
        "        layer_output = x\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                layer_output = layer(layer_output)\n",
        "            else:\n",
        "                if self.cell_type == 'lstm':\n",
        "                    layer_output, state_h, state_c = layer(layer_output)\n",
        "                    encoder_states.extend([state_h, state_c])\n",
        "                else:  # RNN or GRU\n",
        "                    layer_output, state = layer(layer_output)\n",
        "                    encoder_states.append(state)\n",
        "\n",
        "        return layer_output, encoder_states\n",
        "    def adapt_encoder_states_for_decoder(self, encoder_states):\n",
        "        \"\"\"Adapt encoder states to be compatible with decoder dimensions\"\"\"\n",
        "        decoder_states = []\n",
        "\n",
        "        for state in encoder_states:\n",
        "            if self.cell_type == 'lstm':\n",
        "                state_h, state_c = state\n",
        "                if hasattr(self, 'state_h_adapter'):\n",
        "                    adapted_h = self.state_h_adapter(state_h)\n",
        "                    adapted_c = self.state_c_adapter(state_c)\n",
        "                    decoder_states.append((adapted_h, adapted_c))\n",
        "                else:\n",
        "                    decoder_states.append((state_h, state_c))\n",
        "            else:\n",
        "                if hasattr(self, 'state_adapter'):\n",
        "                    adapted_state = self.state_adapter(state)\n",
        "                    decoder_states.append(adapted_state)\n",
        "                else:\n",
        "                    decoder_states.append(state)\n",
        "\n",
        "        return decoder_states\n",
        "\n",
        "    def decode_step(self, x, states,training=None):\n",
        "        \"\"\"\n",
        "        Perform one decoding step.\n",
        "\n",
        "        Args:\n",
        "            x: Input character tensor of shape (batch_size, 1)\n",
        "            states: Previous states from the decoder\n",
        "\n",
        "        Returns:\n",
        "            output: Output probabilities\n",
        "            new_states: Updated states\n",
        "        \"\"\"\n",
        "        # Embed input character\n",
        "\n",
        "        x = self.target_embedding(x)\n",
        "        x = self.embedding_projection(x)\n",
        "\n",
        "        # Process through decoder layers\n",
        "        all_new_states = []\n",
        "        layer_output = x\n",
        "\n",
        "        # Track which state to use for each RNN layer\n",
        "        state_idx = 0\n",
        "        layer_idx = 0\n",
        "\n",
        "        for layer in self.decoder_layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                layer_output = layer(layer_output,training=False)\n",
        "            else:\n",
        "                # Extract the states for this layer\n",
        "                if self.cell_type == 'lstm':\n",
        "                    layer_states = [states[state_idx], states[state_idx+1]]\n",
        "                    state_idx += 2\n",
        "                else:  # RNN or GRU\n",
        "                    layer_states = [states[state_idx]]\n",
        "                    state_idx += 1\n",
        "\n",
        "                # Process through the layer\n",
        "                if self.cell_type == 'lstm':\n",
        "                    layer_output, state_h, state_c = layer(layer_output, initial_state=layer_states)\n",
        "                    all_new_states.extend([state_h, state_c])\n",
        "                else:  # RNN or GRU\n",
        "                    layer_output, state = layer(layer_output, initial_state=layer_states)\n",
        "                    all_new_states.append(state)\n",
        "\n",
        "                layer_idx += 1\n",
        "\n",
        "        # Apply final dropout if specified\n",
        "        if hasattr(self, 'final_dropout'):\n",
        "            layer_output = self.final_dropout(layer_output)\n",
        "\n",
        "        # Generate output probabilities\n",
        "        output = self.fc(layer_output)\n",
        "\n",
        "        return output, all_new_states\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            inputs: Tuple of (input_seq, target_seq)\n",
        "            training: Whether the model is in training mode\n",
        "\n",
        "        Returns:\n",
        "            outputs: Sequence of output probabilities\n",
        "        \"\"\"\n",
        "        input_seq, target_seq = inputs\n",
        "\n",
        "        # Encode the input sequence\n",
        "        encoder_outputs, encoder_states = self.encode(input_seq)\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize decoder states with encoder states\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Initialize decoder states to zeros\n",
        "        decoder_states = []\n",
        "        for layer in self.decoder_layers:\n",
        "            if isinstance(layer, tf.keras.layers.RNN):  # Could be LSTM, GRU, or SimpleRNN\n",
        "                units = layer.cell.units if hasattr(layer, \"cell\") else layer.units\n",
        "                batch_size = tf.shape(input_seq)[0]\n",
        "                if isinstance(layer.cell, tf.keras.layers.LSTMCell):\n",
        "                    decoder_states.extend([\n",
        "                        tf.zeros((batch_size, units)),  # h\n",
        "                        tf.zeros((batch_size, units))   # c\n",
        "                     ])\n",
        "                else:\n",
        "                    decoder_states.append(tf.zeros((batch_size, units)))\n",
        "\n",
        "\n",
        "        # Teacher forcing: feeding the target as the next input\n",
        "        decoder_inputs = target_seq[:, :-1]  # exclude last character\n",
        "\n",
        "        # Initialize list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        # Process each character in the target sequence\n",
        "        for t in range(decoder_inputs.shape[1]):\n",
        "            # Extract input for this timestep\n",
        "            decoder_input = decoder_inputs[:, t:t+1]\n",
        "\n",
        "            # Perform one decoding step\n",
        "            output, decoder_states = self.decode_step(decoder_input, decoder_states,training=training)\n",
        "\n",
        "            # Store output\n",
        "            outputs.append(output)\n",
        "        output, decoder_states = self.decode_step(decoder_input, decoder_states, training=training)\n",
        "\n",
        "        def build(self, input_shape):\n",
        "          # Example: input_shape = (batch_size, timesteps)\n",
        "            dummy_input = tf.random.uniform((1, input_shape[1]))\n",
        "            dummy_embedding = self.embedding(dummy_input)\n",
        "\n",
        "           # Pass through encoder\n",
        "            _= self.encoder(dummy_embedding)\n",
        "\n",
        "            # Create dummy decoder input with correct shape\n",
        "            decoder_input = tf.random.uniform((1, input_shape[1], self.embedding_dim))\n",
        "            decoder_state = [tf.zeros((1, self.decoder_units)) for _ in range(self.decoder_layers)]\n",
        "\n",
        "            # Pass through decoder (for LSTM, you need two states per layer: h and c)\n",
        "            for layer in self.decoder_layers_list:\n",
        "                if isinstance(layer.cell, tf.keras.layers.LSTMCell):\n",
        "                    state = [tf.zeros((1, self.decoder_units)), tf.zeros((1, self.decoder_units))]\n",
        "                else:\n",
        "                    state = [tf.zeros((1, self.decoder_units))]\n",
        "                _ = layer(decoder_input, initial_state=state)\n",
        "\n",
        "            super().build(input_shape)\n",
        "\n",
        "        # Concatenate outputs along time axis\n",
        "        return tf.concat(outputs, axis=1)\n",
        "\n",
        "    def beam_search_decode(self, input_seq, max_length, beam_size=3, start_token=None, end_token=None):\n",
        "        \"\"\"\n",
        "        Generate a transliteration using beam search.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "            max_length: Maximum length of the output sequence\n",
        "            beam_size: Number of beams to track\n",
        "            start_token: ID of the start token\n",
        "            end_token: ID of the end token\n",
        "\n",
        "        Returns:\n",
        "            best_sequences: List of best sequences found (one per input sequence)\n",
        "        \"\"\"\n",
        "        batch_size = input_seq.shape[0]\n",
        "\n",
        "        # Default tokens if not provided\n",
        "        if start_token is None:\n",
        "            start_token = self.target_vocab_size - 2\n",
        "        if end_token is None:\n",
        "            end_token = self.target_vocab_size - 1\n",
        "\n",
        "        # Encode input sequence\n",
        "        _, encoder_states = self.encode(input_seq)\n",
        "\n",
        "        # Process each input sequence separately (beam search isn't batch-friendly)\n",
        "        best_sequences = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # Extract states for this batch item\n",
        "            if self.cell_type == 'lstm':\n",
        "                batch_states = []\n",
        "                for i in range(0, len(encoder_states), 2):\n",
        "                    batch_states.append(encoder_states[i][b:b+1])\n",
        "                    batch_states.append(encoder_states[i+1][b:b+1])\n",
        "            else:\n",
        "                batch_states = [state[b:b+1] for state in encoder_states]\n",
        "\n",
        "            # Initialize beam with start token\n",
        "            beams = [(0.0, [start_token], batch_states)]\n",
        "            complete_beams = []\n",
        "\n",
        "            # Generate sequence\n",
        "            for t in range(max_length):\n",
        "                new_beams = []\n",
        "\n",
        "                # Process each beam\n",
        "                for score, sequence, states in beams:\n",
        "                    # Check if sequence is complete\n",
        "                    if sequence[-1] == end_token:\n",
        "                        complete_beams.append((score, sequence))\n",
        "                        continue\n",
        "\n",
        "                    # Get next token predictions\n",
        "                    decoder_input = tf.constant([[sequence[-1]]], dtype=tf.int32)\n",
        "                    output, new_states = self.decode_step(decoder_input, states)\n",
        "\n",
        "                    # Get top k predictions\n",
        "                    log_probs = tf.math.log(output[0, 0])\n",
        "                    top_k_probs, top_k_indices = tf.math.top_k(log_probs, k=beam_size)\n",
        "\n",
        "                    # Create new beams\n",
        "                    for i in range(beam_size):\n",
        "                        new_score = score + top_k_probs[i].numpy()\n",
        "                        new_sequence = sequence + [top_k_indices[i].numpy()]\n",
        "                        new_beams.append((new_score, new_sequence, new_states))\n",
        "\n",
        "                # Keep only the best beams\n",
        "                beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
        "\n",
        "                # Early stopping if all beams are complete\n",
        "                if all(beam[1][-1] == end_token for beam in beams):\n",
        "                    complete_beams.extend(beams)\n",
        "                    break\n",
        "\n",
        "            # Add incomplete beams to complete ones\n",
        "            complete_beams.extend([(score, sequence) for score, sequence, _ in beams if sequence[-1] != end_token])\n",
        "\n",
        "            # Sort and select the best sequence\n",
        "            if complete_beams:\n",
        "                best_sequence = sorted(complete_beams, key=lambda x: x[0], reverse=True)[0][1]\n",
        "                # Remove start and end tokens\n",
        "                if best_sequence[-1] == end_token:\n",
        "                    best_sequence = best_sequence[1:-1]\n",
        "                else:\n",
        "                    best_sequence = best_sequence[1:]\n",
        "            else:\n",
        "                best_sequence = []\n",
        "\n",
        "            best_sequences.append(best_sequence)\n",
        "\n",
        "        return best_sequences\n",
        "\n",
        "# Decoding and evaluation functions\n",
        "def decode_sequences(model, sequences, start_token, end_token, max_length, beam_size=1):\n",
        "    \"\"\"\n",
        "    Decode sequences using the model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        sequences: Input sequences to decode\n",
        "        start_token, end_token: Tokens to mark sequence start/end\n",
        "        max_length: Maximum output sequence length\n",
        "        beam_size: Beam size for beam search (1 = greedy)\n",
        "\n",
        "    Returns:\n",
        "        predicted_sequences: List of predicted token sequences\n",
        "    \"\"\"\n",
        "    if beam_size > 1:\n",
        "        return model.beam_search_decode(\n",
        "            sequences,\n",
        "            max_length=max_length,\n",
        "            beam_size=beam_size,\n",
        "            start_token=start_token,\n",
        "            end_token=end_token\n",
        "        )\n",
        "    else:\n",
        "        # Initialize with start token\n",
        "        batch_size = sequences.shape[0]\n",
        "        current_tokens = tf.ones((batch_size, 1), dtype=tf.int32) * start_token\n",
        "        predicted_sequences = []\n",
        "\n",
        "        # Encode input\n",
        "        _, encoder_states = model.encode(sequences)\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Generate tokens one by one\n",
        "        for t in range(max_length):\n",
        "            # Predict next token\n",
        "            output, decoder_states = model.decode_step(current_tokens, decoder_states)\n",
        "            predicted_token = tf.argmax(output, axis=-1)\n",
        "\n",
        "            # Store prediction\n",
        "            predicted_sequences.append(predicted_token)\n",
        "\n",
        "            # Check for end tokens\n",
        "            if tf.reduce_all(predicted_token == end_token):\n",
        "                break\n",
        "\n",
        "            # Update current token\n",
        "            current_tokens = predicted_token\n",
        "\n",
        "        # Concatenate and convert to list\n",
        "        predicted_sequences = tf.concat(predicted_sequences, axis=1).numpy()\n",
        "\n",
        "        # Remove end tokens and convert to list\n",
        "        result = []\n",
        "        for seq in predicted_sequences:\n",
        "            # Find position of end token\n",
        "            try:\n",
        "                end_pos = list(seq).index(end_token)\n",
        "                result.append(list(seq[:end_pos]))\n",
        "            except ValueError:\n",
        "                # No end token found\n",
        "                result.append(list(seq))\n",
        "\n",
        "        return result\n",
        "\n",
        "def calculate_accuracy(true_seqs, pred_seqs):\n",
        "    \"\"\"\n",
        "    Calculate character and word accuracy between true and predicted sequences.\n",
        "\n",
        "    Args:\n",
        "        true_seqs: List of true sequences\n",
        "        pred_seqs: List of predicted sequences\n",
        "\n",
        "    Returns:\n",
        "        Dict with character and word level accuracy\n",
        "    \"\"\"\n",
        "    char_correct = 0\n",
        "    char_total = 0\n",
        "    word_correct = 0\n",
        "    word_total = len(true_seqs)\n",
        "\n",
        "    for true_seq, pred_seq in zip(true_seqs, pred_seqs):\n",
        "        # Word is correct if all characters match\n",
        "        word_is_correct = True\n",
        "\n",
        "        # Count character matches\n",
        "        for i in range(min(len(true_seq), len(pred_seq))):\n",
        "            if true_seq[i] == pred_seq[i]:\n",
        "                char_correct += 1\n",
        "            else:\n",
        "                word_is_correct = False\n",
        "\n",
        "        # Add extra characters as errors\n",
        "        if len(true_seq) != len(pred_seq):\n",
        "            word_is_correct = False\n",
        "\n",
        "        # Count total characters\n",
        "        char_total += max(len(true_seq), len(pred_seq))\n",
        "\n",
        "        # Update word accuracy\n",
        "        if word_is_correct:\n",
        "            word_correct += 1\n",
        "\n",
        "    return {\n",
        "        'char_accuracy': char_correct / char_total if char_total > 0 else 0,\n",
        "        'word_accuracy': word_correct / word_total if word_total > 0 else 0\n",
        "    }\n",
        "\n",
        "def tokens_to_text(sequences, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert token sequences back to text.\n",
        "\n",
        "    Args:\n",
        "        sequences: List of token sequences\n",
        "        tokenizer: Keras tokenizer\n",
        "\n",
        "    Returns:\n",
        "        List of text sequences\n",
        "    \"\"\"\n",
        "    # Reverse the word index\n",
        "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "    # Convert sequences to text\n",
        "    texts = []\n",
        "    for seq in sequences:\n",
        "        text = ''.join(index_word.get(token, '') for token in seq if token in index_word)\n",
        "        texts.append(text)\n",
        "\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "hzLv-_I-wQ4-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HD_ONk_wqFNr",
        "outputId": "c0d962e1-cdfa-4dd9-9d4c-bc0cbd545acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 0ftoj7ow\n",
            "Sweep URL: https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jxnu41s9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_hidden_dim: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_units: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_decoder_units_pair: {'decoder_units': 256, 'encoder_units': 256}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_units: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout_rate: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250518_090727-jxnu41s9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/jxnu41s9' target=\"_blank\">fast-sweep-1</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/jxnu41s9' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/jxnu41s9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 106260 training, 10424 dev, and 10517 test examples.\n",
            "  native latin  n\n",
            "0        aao  3\n",
            "1       aaoo  1\n",
            "2        aau  1\n",
            "3        aaw  1\n",
            "4       aawo  1\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'transliteration_seq2_seq' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Exception encountered when calling GRU.call().\n",
            "\n",
            "\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n",
            "\n",
            "Arguments received by GRU.call():\n",
            "   sequences=tf.Tensor(shape=(None, 23, 64), dtype=float32)\n",
            "   initial_state=None\n",
            "   mask=None\n",
            "   training=False''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transliteration_seq2_seq', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fast-sweep-1</strong> at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/jxnu41s9' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/jxnu41s9</a><br> View project at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250518_090727-jxnu41s9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jxnu41s9 errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-6-368e14052b3f>\", line 160, in train_sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     history = model.fit(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-d138a4a49ecf>\", line 224, in call\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     encoder_outputs, encoder_states = self.encode(input_seq)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                       ^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-d138a4a49ecf>\", line 128, in encode\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     layer_output, state = layer(layer_output)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                           ^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: Exception encountered when calling GRU.call().\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Arguments received by GRU.call():\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m    sequences=tf.Tensor(shape=(None, 23, 64), dtype=float32)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m    initial_state=None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m    mask=None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m    training=True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ptw9jjle with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_hidden_dim: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_units: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_decoder_units_pair: {'decoder_units': 64, 'encoder_units': 64}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_units: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout_rate: 0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250518_090739-ptw9jjle</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ptw9jjle' target=\"_blank\">wobbly-sweep-2</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/0ftoj7ow</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ptw9jjle' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ptw9jjle</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 106260 training, 10424 dev, and 10517 test examples.\n",
            "  native latin  n\n",
            "0        aao  3\n",
            "1       aaoo  1\n",
            "2        aau  1\n",
            "3        aaw  1\n",
            "4       aawo  1\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6742 - loss: 1.2800Epoch 1/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 31ms/step - accuracy: 0.6742 - loss: 1.2799 - val_accuracy: 0.7059 - val_loss: 1.0623\n",
            "Epoch 2/20\n",
            "\u001b[1m 419/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.7003 - loss: 1.0783"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transliteration_seq2_seq', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - accuracy: 0.7010 - loss: 1.0740 - val_accuracy: 0.7078 - val_loss: 1.0464\n",
            "Epoch 3/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.7027 - loss: 1.0579 - val_accuracy: 0.7095 - val_loss: 1.0401\n",
            "Epoch 4/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.7047 - loss: 1.0485 - val_accuracy: 0.7093 - val_loss: 1.0331\n",
            "Epoch 5/20\n",
            "\u001b[1m 731/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.7055 - loss: 1.0425"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['transliteration_seq2_seq/embedding_1/embeddings', 'transliteration_seq2_seq/encoder_layer_1/lstm_cell/kernel', 'transliteration_seq2_seq/encoder_layer_1/lstm_cell/recurrent_kernel', 'transliteration_seq2_seq/encoder_layer_1/lstm_cell/bias', 'transliteration_seq2_seq/encoder_layer_2/lstm_cell/kernel', 'transliteration_seq2_seq/encoder_layer_2/lstm_cell/recurrent_kernel', 'transliteration_seq2_seq/encoder_layer_2/lstm_cell/bias', 'transliteration_seq2_seq/encoder_layer_3/lstm_cell/kernel', 'transliteration_seq2_seq/encoder_layer_3/lstm_cell/recurrent_kernel', 'transliteration_seq2_seq/encoder_layer_3/lstm_cell/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.7055 - loss: 1.0419 - val_accuracy: 0.7120 - val_loss: 1.0270\n",
            "Epoch 6/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.7061 - loss: 1.0365 - val_accuracy: 0.7113 - val_loss: 1.0236\n",
            "Epoch 7/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.7068 - loss: 1.0320 - val_accuracy: 0.7124 - val_loss: 1.0204\n",
            "Epoch 8/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.7074 - loss: 1.0298 - val_accuracy: 0.7111 - val_loss: 1.0198\n",
            "Epoch 9/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7079 - loss: 1.0275 - val_accuracy: 0.7116 - val_loss: 1.0169\n",
            "Epoch 10/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7086 - loss: 1.0259 - val_accuracy: 0.7122 - val_loss: 1.0172\n",
            "Epoch 11/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7076 - loss: 1.0251 - val_accuracy: 0.7140 - val_loss: 1.0163\n",
            "Epoch 12/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7083 - loss: 1.0238 - val_accuracy: 0.7123 - val_loss: 1.0178\n",
            "Epoch 13/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7083 - loss: 1.0225 - val_accuracy: 0.7131 - val_loss: 1.0161\n",
            "Epoch 14/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 29ms/step - accuracy: 0.7087 - loss: 1.0217 - val_accuracy: 0.7132 - val_loss: 1.0160\n",
            "Epoch 15/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 28ms/step - accuracy: 0.7092 - loss: 1.0199 - val_accuracy: 0.7145 - val_loss: 1.0144\n",
            "Epoch 16/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 27ms/step - accuracy: 0.7086 - loss: 1.0226 - val_accuracy: 0.7143 - val_loss: 1.0143\n",
            "Epoch 17/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 30ms/step - accuracy: 0.7089 - loss: 1.0205 - val_accuracy: 0.7140 - val_loss: 1.0141\n",
            "Epoch 18/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 29ms/step - accuracy: 0.7092 - loss: 1.0185 - val_accuracy: 0.7132 - val_loss: 1.0146\n",
            "Epoch 19/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 28ms/step - accuracy: 0.7095 - loss: 1.0189 - val_accuracy: 0.7137 - val_loss: 1.0141\n",
            "Epoch 20/20\n",
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 30ms/step - accuracy: 0.7094 - loss: 1.0174 - val_accuracy: 0.7141 - val_loss: 1.0136\n",
            "\u001b[1m1425/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:18\u001b[0m 332ms/step - accuracy: 0.6691 - loss: 1.3377"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1427/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:17\u001b[0m 332ms/step - accuracy: 0.6691 - loss: 1.3375"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wobbly-sweep-2</strong> at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ptw9jjle' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ptw9jjle</a><br> View project at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250518_090739-ptw9jjle/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1661/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 348ms/step - accuracy: 0.6723 - loss: 1.3112 - val_accuracy: 0.7035 - val_loss: 1.0661\n",
            "Epoch 2/20\n",
            "\u001b[1m1517/1661\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 293ms/step - accuracy: 0.7052 - loss: 1.0501"
          ]
        }
      ],
      "source": [
        "# Model creation with dropout support\n",
        "def create_model(input_vocab_size,\n",
        "                target_vocab_size,\n",
        "                embedding_dim,\n",
        "                decoder_hidden_dim,\n",
        "                encoder_units,\n",
        "                decoder_units,\n",
        "                cell_type='lstm',\n",
        "                encoder_layers=1,\n",
        "                decoder_layers=1,\n",
        "                dropout_rate=0.0,\n",
        "                recurrent_dropout_rate=0.0):\n",
        "    \"\"\"\n",
        "    Create and compile a transliteration model.\n",
        "\n",
        "    Args:\n",
        "        input_vocab_size: Size of the input vocabulary\n",
        "        target_vocab_size: Size of the target vocabulary\n",
        "        embedding_dim: Dimension of character embeddings\n",
        "        encoder_units: Number of units in encoder cell\n",
        "        decoder_units: Number of units in decoder cell\n",
        "        cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "        encoder_layers: Number of layers in the encoder\n",
        "        decoder_layers: Number of layers in the decoder\n",
        "        dropout_rate: Dropout rate after RNN layers\n",
        "        recurrent_dropout_rate: Dropout rate inside RNN cells\n",
        "\n",
        "    Returns:\n",
        "        model: Compiled transliteration model\n",
        "    \"\"\"\n",
        "    # Create inputs\n",
        "    input_seq = Input(shape=(None,), name='input_sequence')\n",
        "    target_seq = Input(shape=(None,), name='target_sequence')\n",
        "\n",
        "    # Create model\n",
        "    model = TransliterationSeq2Seq(\n",
        "        input_vocab_size=input_vocab_size,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        decoder_hidden_dim=decoder_hidden_dim,\n",
        "        encoder_units=encoder_units,\n",
        "        decoder_units=decoder_units,\n",
        "        cell_type=cell_type,\n",
        "        encoder_layers=encoder_layers,\n",
        "        decoder_layers=decoder_layers,\n",
        "        dropout_rate=dropout_rate,\n",
        "        recurrent_dropout_rate=recurrent_dropout_rate\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# WandB setup and hyperparameter sweep\n",
        "def setup_wandb_sweep():\n",
        "    \"\"\"\n",
        "    Setup wandb hyperparameter sweep.\n",
        "\n",
        "    Returns:\n",
        "        sweep_id: ID of the created sweep\n",
        "    \"\"\"\n",
        "    sweep_config = {\n",
        "        'method': 'bayes',  # Bayesian optimization\n",
        "        'metric': {\n",
        "            'name': 'val_accuracy',\n",
        "            'goal': 'maximize'\n",
        "        },\n",
        "        'parameters': {\n",
        "            'embedding_dim': {\n",
        "                'values': [16, 32, 64]\n",
        "            },\n",
        "            'decoder_hidden_dim': {\n",
        "                'values': [16, 32, 64]\n",
        "            },\n",
        "            'encoder_units': {\n",
        "                'values': [32, 64, 128, 256]\n",
        "            },\n",
        "            'decoder_units': {\n",
        "                'values': [32, 64, 128, 256]\n",
        "            },\n",
        "\n",
        "            \"encoder_decoder_units_pair\": {\n",
        "            \"values\": [\n",
        "                {\"encoder_units\": 32, \"decoder_units\": 32},\n",
        "                {\"encoder_units\": 64, \"decoder_units\": 64},\n",
        "                {\"encoder_units\": 128, \"decoder_units\": 128},\n",
        "                {\"encoder_units\": 256, \"decoder_units\": 256}\n",
        "            ]},\n",
        "            'encoder_layers': {\n",
        "                'values': [1, 2, 3]\n",
        "            },\n",
        "            'decoder_layers': {\n",
        "                'values': [1, 2, 3]\n",
        "            },\n",
        "            'cell_type': {\n",
        "                'values': ['lstm', 'gru', 'rnn']\n",
        "            },\n",
        "            'dropout_rate': {\n",
        "                'values': [0.0, 0.2, 0.3]\n",
        "            },\n",
        "            'recurrent_dropout_rate': {\n",
        "                'values': [0.0, 0.1, 0.2]\n",
        "            },\n",
        "            'beam_size': {\n",
        "                'values': [1, 3, 5]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create sweep\n",
        "    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration\")\n",
        "\n",
        "    return sweep_id\n",
        "\n",
        "\n",
        "# Training function for wandb sweep\n",
        "def train_sweep():\n",
        "    \"\"\"\n",
        "    Training function for wandb sweep.\n",
        "    \"\"\"\n",
        "    # Initialize wandb\n",
        "    run = wandb.init()\n",
        "\n",
        "    # Get wandb config\n",
        "    config = wandb.config\n",
        "\n",
        "    # Load data\n",
        "    train_data, dev_data, test_data = load_dakshina_data()\n",
        "    preprocessed_data = preprocess_data(train_data, dev_data, test_data)\n",
        "\n",
        "    # Create model with config parameters\n",
        "    model = create_model(\n",
        "        input_vocab_size=preprocessed_data['latin_vocab_size'],\n",
        "        target_vocab_size=preprocessed_data['native_vocab_size'],\n",
        "        embedding_dim=config.embedding_dim,\n",
        "        decoder_hidden_dim=config.decoder_hidden_dim,\n",
        "        encoder_units=config.encoder_units,\n",
        "        decoder_units=config.decoder_units,\n",
        "        cell_type=config.cell_type,\n",
        "        encoder_layers=config.encoder_layers,\n",
        "        decoder_layers=config.decoder_layers,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "        recurrent_dropout_rate=config.recurrent_dropout_rate\n",
        "    )\n",
        "\n",
        "    # # Setup callbacks\n",
        "    # callbacks = [\n",
        "    #     WandbCallback(),\n",
        "    #     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    #     ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "    # ]\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        [preprocessed_data['train_latin'], preprocessed_data['train_native']],\n",
        "        preprocessed_data['train_native_target'],\n",
        "        validation_data=(\n",
        "            [preprocessed_data['dev_latin'], preprocessed_data['dev_native']],\n",
        "            preprocessed_data['dev_native_target']\n",
        "        ),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        # callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Evaluate with beam search\n",
        "    beam_size = config.beam_size\n",
        "\n",
        "    # Decode dev set with beam search\n",
        "    pred_dev_sequences = decode_sequences(\n",
        "        model,\n",
        "        preprocessed_data['dev_latin'],\n",
        "        preprocessed_data['start_token'],\n",
        "        preprocessed_data['end_token'],\n",
        "        preprocessed_data['max_native_len'],\n",
        "        beam_size=beam_size\n",
        "    )\n",
        "\n",
        "    # Remove start/end tokens from the target sequences\n",
        "    true_dev_sequences = [seq[1:-1] for seq in preprocessed_data['dev_native'].tolist()]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = calculate_accuracy(true_dev_sequences, pred_dev_sequences)\n",
        "\n",
        "    # Log metrics\n",
        "    wandb.log({\n",
        "        'dev_char_accuracy': accuracy['char_accuracy'],\n",
        "        'dev_word_accuracy': accuracy['word_accuracy']\n",
        "    })\n",
        "\n",
        "    # Convert some examples to text for visual inspection\n",
        "    native_tokenizer = preprocessed_data['native_tokenizer']\n",
        "    latin_tokenizer = preprocessed_data['latin_tokenizer']\n",
        "\n",
        "    # Get sample predictions\n",
        "    sample_idx = np.random.choice(len(pred_dev_sequences), min(10, len(pred_dev_sequences)), replace=False)\n",
        "    samples = []\n",
        "\n",
        "    for idx in sample_idx:\n",
        "        input_text = tokens_to_text([preprocessed_data['dev_latin'][idx].tolist()], latin_tokenizer)[0]\n",
        "        true_text = tokens_to_text([true_dev_sequences[idx]], native_tokenizer)[0]\n",
        "        pred_text = tokens_to_text([pred_dev_sequences[idx]], native_tokenizer)[0]\n",
        "\n",
        "        samples.append({\n",
        "            'input': input_text,\n",
        "            'true': true_text,\n",
        "            'pred': pred_text\n",
        "        })\n",
        "\n",
        "    # Log sample predictions\n",
        "    wandb.log({'samples': wandb.Table(\n",
        "        columns=['Input', 'True', 'Predicted'],\n",
        "        data=[[s['input'], s['true'], s['pred']] for s in samples]\n",
        "    )})\n",
        "\n",
        "    # Clean up\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "# Main script for running the sweep\n",
        "def main():\n",
        "    \"\"\"Main function to run the sweep.\"\"\"\n",
        "    sweep_id = setup_wandb_sweep()\n",
        "    wandb.agent(sweep_id, train_sweep, count=20)  # Run 20 experiments\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e4d0a8c3ccaf2534e9ab91c659e420ba5114533f"
      ],
      "metadata": {
        "id": "MV5oW4cxJHDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSBKhHWurtqH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPJG4TFFl9cS0mGwbdQCK1a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}