{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazal735/DL_A3/blob/main/DL_A3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JW9jUy9HnQqZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, RNN, LSTM, GRU, SimpleRNN\n",
        "\n",
        "\n",
        "class TransliterationSeq2Seq(Model):\n",
        "    def __init__(self,\n",
        "                 input_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 embedding_dim,\n",
        "                 encoder_units,\n",
        "                 decoder_units,\n",
        "                 cell_type='lstm',\n",
        "                 encoder_layers=1,\n",
        "                 decoder_layers=1):\n",
        "        \"\"\"\n",
        "        Initialize the Seq2Seq model for transliteration.\n",
        "\n",
        "        Args:\n",
        "            input_vocab_size: Size of the input language vocabulary\n",
        "            target_vocab_size: Size of the target language vocabulary\n",
        "            embedding_dim: Dimension of character embeddings\n",
        "            encoder_units: Number of units in encoder cell\n",
        "            decoder_units: Number of units in decoder cell\n",
        "            cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "            encoder_layers: Number of layers in the encoder\n",
        "            decoder_layers: Number of layers in the decoder\n",
        "        \"\"\"\n",
        "        super(TransliterationSeq2Seq, self).__init__()\n",
        "\n",
        "        # Store model parameters\n",
        "        self.encoder_units = encoder_units\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "\n",
        "        # Create embeddings for input characters\n",
        "        self.embedding = Embedding(input_vocab_size, embedding_dim)\n",
        "\n",
        "        # Setup the cell type based on user preference\n",
        "        self.cell_type = cell_type.lower()\n",
        "        if self.cell_type == 'lstm':\n",
        "            rnn_cell = LSTM\n",
        "        elif self.cell_type == 'gru':\n",
        "            rnn_cell = GRU\n",
        "        elif self.cell_type == 'rnn':\n",
        "            rnn_cell = SimpleRNN\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        # Create encoder layers\n",
        "        self.encoder_layers = []\n",
        "        for i in range(encoder_layers):\n",
        "            input_size = embedding_dim if i == 0 else encoder_units\n",
        "            return_sequences = (i < encoder_layers - 1)\n",
        "            self.encoder_layers.append(\n",
        "                rnn_cell(encoder_units,\n",
        "                         return_sequences=return_sequences,\n",
        "                         return_state=True,\n",
        "                         name=f'encoder_layer_{i+1}')\n",
        "            )\n",
        "\n",
        "        # Create decoder layers\n",
        "        self.decoder_layers = []\n",
        "        for i in range(decoder_layers):\n",
        "            input_size = embedding_dim if i == 0 else decoder_units\n",
        "            return_sequences = True\n",
        "            self.decoder_layers.append(\n",
        "                rnn_cell(decoder_units,\n",
        "                         return_sequences=return_sequences,\n",
        "                         return_state=True,\n",
        "                         name=f'decoder_layer_{i+1}')\n",
        "            )\n",
        "\n",
        "        # Output layer to convert decoder output to character probabilities\n",
        "        self.fc = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "        if encoder_units != decoder_units:\n",
        "            if self.cell_type == 'lstm':\n",
        "                self.state_h_adapter = tf.keras.layers.Dense(decoder_units, name='state_h_adapter')\n",
        "                self.state_c_adapter = tf.keras.layers.Dense(decoder_units, name='state_c_adapter')\n",
        "            else:\n",
        "                self.state_adapter = tf.keras.layers.Dense(decoder_units, name='state_adapter')\n",
        "\n",
        "    def encode(self, input_seq):\n",
        "        \"\"\"\n",
        "        Encode the input sequence.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "\n",
        "        Returns:\n",
        "            encoder_outputs: Outputs from the encoder\n",
        "            encoder_states: Final states from the encoder (to initialize decoder)\n",
        "        \"\"\"\n",
        "        # Embed input sequence\n",
        "        x = self.embedding(input_seq)\n",
        "\n",
        "        # Process through encoder layers\n",
        "        encoder_states = []\n",
        "        for i, encoder_layer in enumerate(self.encoder_layers):\n",
        "            if i == 0:\n",
        "                outputs = x\n",
        "\n",
        "            if self.cell_type == 'lstm':\n",
        "                outputs, state_h, state_c = encoder_layer(outputs)\n",
        "                encoder_states.extend([state_h, state_c])\n",
        "            else:  # RNN or GRU\n",
        "                outputs, state = encoder_layer(outputs)\n",
        "                encoder_states.append(state)\n",
        "\n",
        "        return outputs, encoder_states\n",
        "\n",
        "    def adapt_encoder_states_for_decoder(self, encoder_states):\n",
        "        \"\"\"Adapt encoder states to be compatible with decoder dimensions\"\"\"\n",
        "        decoder_states = []\n",
        "\n",
        "        for state in encoder_states:\n",
        "            if self.cell_type == 'lstm':\n",
        "                state_h, state_c = state\n",
        "                if hasattr(self, 'state_h_adapter'):\n",
        "                    adapted_h = self.state_h_adapter(state_h)\n",
        "                    adapted_c = self.state_c_adapter(state_c)\n",
        "                    decoder_states.append((adapted_h, adapted_c))\n",
        "                else:\n",
        "                    decoder_states.append((state_h, state_c))\n",
        "            else:\n",
        "                if hasattr(self, 'state_adapter'):\n",
        "                    adapted_state = self.state_adapter(state)\n",
        "                    decoder_states.append(adapted_state)\n",
        "                else:\n",
        "                    decoder_states.append(state)\n",
        "\n",
        "        return decoder_states\n",
        "\n",
        "    def decode_step(self, x, states):\n",
        "        \"\"\"\n",
        "        Perform one decoding step.\n",
        "\n",
        "        Args:\n",
        "            x: Input character tensor of shape (batch_size, 1)\n",
        "            states: Previous states from the decoder\n",
        "\n",
        "        Returns:\n",
        "            output: Output probabilities\n",
        "            new_states: Updated states\n",
        "        \"\"\"\n",
        "        # Embed input character\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Process through decoder layers\n",
        "        all_new_states = []\n",
        "        for i, decoder_layer in enumerate(self.decoder_layers):\n",
        "            if i == 0:\n",
        "                layer_input = x\n",
        "            else:\n",
        "                layer_input = outputs\n",
        "\n",
        "            # Extract the states for this layer\n",
        "            if self.cell_type == 'lstm':\n",
        "                layer_states = [states[i*2], states[i*2+1]]\n",
        "            else:  # RNN or GRU\n",
        "                layer_states = [states[i]]\n",
        "\n",
        "            # Process through the layer\n",
        "            if self.cell_type == 'lstm':\n",
        "                outputs, state_h, state_c = decoder_layer(layer_input, initial_state=layer_states)\n",
        "                all_new_states.extend([state_h, state_c])\n",
        "            else:  # RNN or GRU\n",
        "                outputs, state = decoder_layer(layer_input, initial_state=layer_states)\n",
        "                all_new_states.append(state)\n",
        "\n",
        "        # Generate output probabilities\n",
        "        output = self.fc(outputs)\n",
        "\n",
        "        return output, all_new_states\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            inputs: Tuple of (input_seq, target_seq)\n",
        "            training: Whether the model is in training mode\n",
        "\n",
        "        Returns:\n",
        "            outputs: Sequence of output probabilities\n",
        "        \"\"\"\n",
        "        input_seq, target_seq = inputs\n",
        "\n",
        "        # Encode the input sequence\n",
        "        encoder_outputs, encoder_states = self.encode(input_seq)\n",
        "\n",
        "        # Initialize decoder states with encoder states\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Teacher forcing: feeding the target as the next input\n",
        "        decoder_inputs = target_seq[:, :-1]  # exclude last character\n",
        "\n",
        "        # Initialize list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        # Process each character in the target sequence\n",
        "        for t in range(decoder_inputs.shape[1]):\n",
        "            # Extract input for this timestep\n",
        "            decoder_input = decoder_inputs[:, t:t+1]\n",
        "\n",
        "            # Perform one decoding step\n",
        "            output, decoder_states = self.decode_step(decoder_input, decoder_states)\n",
        "\n",
        "            # Store output\n",
        "            outputs.append(output)\n",
        "\n",
        "        # Concatenate outputs along time axis\n",
        "        return tf.concat(outputs, axis=1)\n",
        "\n",
        "    def predict(self, input_seq, max_length=100, start_token=1, end_token=2):\n",
        "        \"\"\"\n",
        "        Generate a transliteration for an input sequence.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "            max_length: Maximum length of the output sequence\n",
        "            start_token: ID of the start token\n",
        "            end_token: ID of the end token\n",
        "\n",
        "        Returns:\n",
        "            outputs: Generated output sequence\n",
        "        \"\"\"\n",
        "        # Encode the input sequence\n",
        "        _, encoder_states = self.encode(input_seq)\n",
        "\n",
        "        # Initialize decoder states with encoder states\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Start with the start token\n",
        "        batch_size = input_seq.shape[0]\n",
        "        decoder_input = tf.ones((batch_size, 1), dtype=tf.int32) * start_token\n",
        "\n",
        "        # Initialize list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        # Generate characters until max_length or end token\n",
        "        for t in range(max_length):\n",
        "            # Perform one decoding step\n",
        "            output, decoder_states = self.decode_step(decoder_input, decoder_states)\n",
        "\n",
        "            # Get the most likely character\n",
        "            predicted_id = tf.argmax(output, axis=-1)\n",
        "\n",
        "            # Store output\n",
        "            outputs.append(predicted_id)\n",
        "\n",
        "            # Break if end token is predicted\n",
        "            if predicted_id == end_token:\n",
        "                break\n",
        "\n",
        "            # Use predicted ID as next input\n",
        "            decoder_input = predicted_id\n",
        "\n",
        "        # Concatenate outputs along time axis\n",
        "        return tf.concat(outputs, axis=1)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "def create_model(input_vocab_size=1000,\n",
        "                target_vocab_size=1000,\n",
        "                embedding_dim=256,\n",
        "                encoder_units=512,\n",
        "                decoder_units=512,\n",
        "                cell_type='lstm',\n",
        "                encoder_layers=1,\n",
        "                decoder_layers=1):\n",
        "    \"\"\"\n",
        "    Create and compile a transliteration model.\n",
        "\n",
        "    Args:\n",
        "        input_vocab_size: Size of the input vocabulary\n",
        "        target_vocab_size: Size of the target vocabulary\n",
        "        embedding_dim: Dimension of character embeddings\n",
        "        encoder_units: Number of units in encoder cell\n",
        "        decoder_units: Number of units in decoder cell\n",
        "        cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "        encoder_layers: Number of layers in the encoder\n",
        "        decoder_layers: Number of layers in the decoder\n",
        "\n",
        "    Returns:\n",
        "        model: Compiled transliteration model\n",
        "    \"\"\"\n",
        "    # Create inputs\n",
        "    input_seq = Input(shape=(None,), name='input_sequence')\n",
        "    target_seq = Input(shape=(None,), name='target_sequence')\n",
        "\n",
        "    # Create model\n",
        "    model = TransliterationSeq2Seq(\n",
        "        input_vocab_size=input_vocab_size,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        encoder_units=encoder_units,\n",
        "        decoder_units=decoder_units,\n",
        "        cell_type=cell_type,\n",
        "        encoder_layers=encoder_layers,\n",
        "        decoder_layers=decoder_layers\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Training example\n",
        "def train_model(model, input_sequences, target_sequences, epochs=10, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train the transliteration model.\n",
        "\n",
        "    Args:\n",
        "        model: Compiled transliteration model\n",
        "        input_sequences: Input sequences (Latin script)\n",
        "        target_sequences: Target sequences (Devanagari script)\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Batch size for training\n",
        "\n",
        "    Returns:\n",
        "        history: Training history\n",
        "    \"\"\"\n",
        "    # Prepare target sequences for training (shifted by 1)\n",
        "    shifted_target_sequences = target_sequences[:, 1:]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [input_sequences, target_sequences],\n",
        "        shifted_target_sequences,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h19j_cRLtYZe",
        "outputId": "accd6fe0-e8fc-4e70-e595-ee1637be2668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: wandb 0.19.11\n",
            "Uninstalling wandb-0.19.11:\n",
            "  Successfully uninstalled wandb-0.19.11\n",
            "\u001b[33mWARNING: Skipping wandb-sdk as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping wandb-core as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip uninstall -y wandb\n",
        "!pip uninstall -y wandb-sdk\n",
        "!pip uninstall -y wandb-core\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSyFN5X2wEL6",
        "outputId": "bd0c16e8-fee1-4cfe-c22d-895b4a22cccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Using cached wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Using cached wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
            "Installing collected packages: wandb\n",
            "Successfully installed wandb-0.19.11\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, RNN, LSTM, GRU, SimpleRNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRh80Y_cDGDK",
        "outputId": "9db46d25-b4b1-40cc-9f72-c1e0ec7095ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and preprocessing\n",
        "\n",
        "\n",
        "def load_dakshina_data(language='ur', data_dir='/content/drive/MyDrive/dakshina_dataset_v1.0/'):\n",
        "\n",
        "    data_path = os.path.join(data_dir, language, 'lexicons')\n",
        "\n",
        "    train_file = os.path.join(data_path, f'{language}.translit.sampled.train.tsv')\n",
        "    dev_file = os.path.join(data_path, f'{language}.translit.sampled.dev.tsv')\n",
        "    test_file = os.path.join(data_path, f'{language}.translit.sampled.test.tsv')\n",
        "\n",
        "    train_data = pd.read_csv(train_file, sep='\\t', header=None, names=['native', 'latin','n'])\n",
        "    dev_data = pd.read_csv(dev_file, sep='\\t', header=None, names=['native', 'latin','n'])\n",
        "    test_data = pd.read_csv(test_file, sep='\\t', header=None, names=['native', 'latin','n'])\n",
        "\n",
        "\n",
        "    print(f\"Loaded {len(train_data)} training, {len(dev_data)} dev, and {len(test_data)} test examples.\")\n",
        "\n",
        "    return train_data, dev_data, test_data"
      ],
      "metadata": {
        "id": "lYAp9zQkDMFa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def preprocess_data(train_data, dev_data, test_data):\n",
        "    \"\"\"\n",
        "    Preprocess the data: tokenize, create vocabulary, and convert to sequences.\n",
        "\n",
        "    Args:\n",
        "        train_data, dev_data, test_data: DataFrames with native and latin script pairs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing preprocessed data and tokenizers\n",
        "    \"\"\"\n",
        "\n",
        "    print(train_data.head())\n",
        "\n",
        "    # Clean the 'latin' column: remove NaNs and convert all to strings\n",
        "    train_data['latin'] = train_data['latin'].astype(str).fillna('')\n",
        "\n",
        "    # Or, to be safer and remove completely invalid rows:\n",
        "    train_data = train_data.dropna(subset=['latin'])\n",
        "    train_data['latin'] = train_data['latin'].astype(str)\n",
        "\n",
        "\n",
        "    # Create character-level tokenizers\n",
        "    latin_tokenizer = Tokenizer(char_level=True)\n",
        "    native_tokenizer = Tokenizer(char_level=True)\n",
        "\n",
        "    # Fit tokenizers on training data\n",
        "    latin_tokenizer.fit_on_texts((train_data['latin']).tolist())\n",
        "    native_tokenizer.fit_on_texts((train_data['native']).tolist())\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    latin_vocab_size = len(latin_tokenizer.word_index) + 1  # +1 for padding\n",
        "    native_vocab_size = len(native_tokenizer.word_index) + 1  # +1 for padding\n",
        "\n",
        "    # Convert text to sequences\n",
        "    train_latin_seq = latin_tokenizer.texts_to_sequences(list(train_data['latin']))\n",
        "    train_native_seq = native_tokenizer.texts_to_sequences(list(train_data['native']))\n",
        "\n",
        "    dev_latin_seq = latin_tokenizer.texts_to_sequences(list(dev_data['latin']))\n",
        "    dev_native_seq = native_tokenizer.texts_to_sequences(list(dev_data['native']))\n",
        "\n",
        "    test_latin_seq = latin_tokenizer.texts_to_sequences(list(test_data['latin']))\n",
        "    test_native_seq = native_tokenizer.texts_to_sequences(list(test_data['native']))\n",
        "\n",
        "    # Find maximum sequence lengths\n",
        "    max_latin_len = max(max(len(seq) for seq in train_latin_seq),\n",
        "                        max(len(seq) for seq in dev_latin_seq),\n",
        "                        max(len(seq) for seq in test_latin_seq))\n",
        "\n",
        "    max_native_len = max(max(len(seq) for seq in train_native_seq),\n",
        "                         max(len(seq) for seq in dev_native_seq),\n",
        "                         max(len(seq) for seq in test_native_seq))\n",
        "\n",
        "    # Add 2 for start and end tokens\n",
        "    max_latin_len += 2\n",
        "    max_native_len += 2\n",
        "\n",
        "    # Add start (<s>) and end (</s>) tokens to target sequences\n",
        "    start_token = native_vocab_size  # Use vocab_size as start token\n",
        "    end_token = native_vocab_size + 1  # Use vocab_size+1 as end token\n",
        "    native_vocab_size += 2  # Increase vocab size for start and end tokens\n",
        "\n",
        "    # Add start and end tokens to native sequences\n",
        "    def add_start_end(sequences, max_len, start_token, end_token):\n",
        "        new_sequences = []\n",
        "        for seq in sequences:\n",
        "            new_seq = [start_token] + seq + [end_token]\n",
        "            new_sequences.append(new_seq)\n",
        "        return pad_sequences(new_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Pad sequences\n",
        "    train_latin_padded = pad_sequences(train_latin_seq, maxlen=max_latin_len, padding='post')\n",
        "    train_native_padded = add_start_end(train_native_seq, max_native_len, start_token, end_token)\n",
        "\n",
        "    dev_latin_padded = pad_sequences(dev_latin_seq, maxlen=max_latin_len, padding='post')\n",
        "    dev_native_padded = add_start_end(dev_native_seq, max_native_len, start_token, end_token)\n",
        "\n",
        "    test_latin_padded = pad_sequences(test_latin_seq, maxlen=max_latin_len, padding='post')\n",
        "    test_native_padded = add_start_end(test_native_seq, max_native_len, start_token, end_token)\n",
        "\n",
        "    # Target for teacher forcing (shifted right by one position)\n",
        "    train_native_target = train_native_padded[:, 1:]  # Remove start token\n",
        "    dev_native_target = dev_native_padded[:, 1:]  # Remove start token\n",
        "\n",
        "    return {\n",
        "        'latin_tokenizer': latin_tokenizer,\n",
        "        'native_tokenizer': native_tokenizer,\n",
        "        'latin_vocab_size': latin_vocab_size,\n",
        "        'native_vocab_size': native_vocab_size,\n",
        "        'max_latin_len': max_latin_len,\n",
        "        'max_native_len': max_native_len,\n",
        "        'start_token': start_token,\n",
        "        'end_token': end_token,\n",
        "        'train_latin': train_latin_padded,\n",
        "        'train_native': train_native_padded,\n",
        "        'train_native_target': train_native_target,\n",
        "        'dev_latin': dev_latin_padded,\n",
        "        'dev_native': dev_native_padded,\n",
        "        'dev_native_target': dev_native_target,\n",
        "        'test_latin': test_latin_padded,\n",
        "        'test_native': test_native_padded\n",
        "    }\n"
      ],
      "metadata": {
        "id": "nbQLy9sVDeGz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Seq2Seq model with dropout and beam search\n",
        "class TransliterationSeq2Seq(Model):\n",
        "    def __init__(self,\n",
        "                 input_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 embedding_dim,\n",
        "                 encoder_units,\n",
        "                 decoder_units,\n",
        "                 cell_type='lstm',\n",
        "                 encoder_layers=1,\n",
        "                 decoder_layers=1,\n",
        "                 dropout_rate=0.0,\n",
        "\n",
        "\n",
        "                 recurrent_dropout_rate=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the Seq2Seq model for transliteration with dropout support.\n",
        "\n",
        "        Args:\n",
        "            input_vocab_size: Size of the input language vocabulary\n",
        "            target_vocab_size: Size of the target language vocabulary\n",
        "            embedding_dim: Dimension of character embeddings\n",
        "            encoder_units: Number of units in encoder cell\n",
        "            decoder_units: Number of units in decoder cell\n",
        "            cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "            encoder_layers: Number of layers in the encoder\n",
        "            decoder_layers: Number of layers in the decoder\n",
        "            dropout_rate: Dropout rate after RNN layers\n",
        "            recurrent_dropout_rate: Dropout rate inside RNN cells\n",
        "        \"\"\"\n",
        "        super(TransliterationSeq2Seq, self).__init__()\n",
        "\n",
        "        # Store model parameters\n",
        "        self.encoder_units = encoder_units\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Create embeddings for input and target characters\n",
        "        self.input_embedding = Embedding(input_vocab_size, embedding_dim)\n",
        "        self.target_embedding = Embedding(target_vocab_size, embedding_dim)\n",
        "\n",
        "        # Setup the cell type based on user preference\n",
        "        self.cell_type = cell_type.lower()\n",
        "        rnn_kwargs = {'return_state': True}\n",
        "\n",
        "        # Add recurrent dropout if specified\n",
        "        if recurrent_dropout_rate > 0:\n",
        "            rnn_kwargs['recurrent_dropout'] = recurrent_dropout_rate\n",
        "\n",
        "        if self.cell_type == 'lstm':\n",
        "            rnn_cell = LSTM\n",
        "        elif self.cell_type == 'gru':\n",
        "            rnn_cell = GRU\n",
        "        elif self.cell_type == 'rnn':\n",
        "            rnn_cell = SimpleRNN\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "        # Create encoder layers\n",
        "        self.encoder_layers = []\n",
        "        for i in range(encoder_layers):\n",
        "            return_sequences = (i < encoder_layers - 1)\n",
        "            layer_kwargs = rnn_kwargs.copy()\n",
        "            layer_kwargs['return_sequences'] = return_sequences\n",
        "            layer_kwargs['name'] = f'encoder_layer_{i+1}'\n",
        "\n",
        "            self.encoder_layers.append(rnn_cell(encoder_units, **layer_kwargs))\n",
        "\n",
        "            # Add dropout between layers if specified\n",
        "            if dropout_rate > 0 and i < encoder_layers - 1:\n",
        "                self.encoder_layers.append(Dropout(dropout_rate))\n",
        "\n",
        "        # Create decoder layers\n",
        "        self.decoder_layers = []\n",
        "        for i in range(decoder_layers):\n",
        "            return_sequences = True\n",
        "            layer_kwargs = rnn_kwargs.copy()\n",
        "            layer_kwargs['return_sequences'] = return_sequences\n",
        "            layer_kwargs['name'] = f'decoder_layer_{i+1}'\n",
        "\n",
        "            self.decoder_layers.append(rnn_cell(decoder_units, **layer_kwargs))\n",
        "\n",
        "            # Add dropout between layers if specified\n",
        "            if dropout_rate > 0 and i < decoder_layers - 1:\n",
        "                self.decoder_layers.append(Dropout(dropout_rate))\n",
        "\n",
        "        # Final dropout before output layer\n",
        "        if dropout_rate > 0:\n",
        "            self.final_dropout = Dropout(dropout_rate)\n",
        "\n",
        "        # Output layer to convert decoder output to character probabilities\n",
        "        self.fc = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def encode(self, input_seq):\n",
        "        \"\"\"\n",
        "        Encode the input sequence.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "\n",
        "        Returns:\n",
        "            encoder_outputs: Outputs from the encoder\n",
        "            encoder_states: Final states from the encoder (to initialize decoder)\n",
        "        \"\"\"\n",
        "        # Embed input sequence\n",
        "        x = self.input_embedding(input_seq)\n",
        "\n",
        "        # Process through encoder layers\n",
        "        encoder_states = []\n",
        "        layer_output = x\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                layer_output = layer(layer_output)\n",
        "            else:\n",
        "                if self.cell_type == 'lstm':\n",
        "                    layer_output, state_h, state_c = layer(layer_output)\n",
        "                    encoder_states.extend([state_h, state_c])\n",
        "                else:  # RNN or GRU\n",
        "                    layer_output, state = layer(layer_output)\n",
        "                    encoder_states.append(state)\n",
        "\n",
        "        return layer_output, encoder_states\n",
        "    def adapt_encoder_states_for_decoder(self, encoder_states):\n",
        "        \"\"\"Adapt encoder states to be compatible with decoder dimensions\"\"\"\n",
        "        decoder_states = []\n",
        "\n",
        "        for state in encoder_states:\n",
        "            if self.cell_type == 'lstm':\n",
        "                state_h, state_c = state\n",
        "                if hasattr(self, 'state_h_adapter'):\n",
        "                    adapted_h = self.state_h_adapter(state_h)\n",
        "                    adapted_c = self.state_c_adapter(state_c)\n",
        "                    decoder_states.append((adapted_h, adapted_c))\n",
        "                else:\n",
        "                    decoder_states.append((state_h, state_c))\n",
        "            else:\n",
        "                if hasattr(self, 'state_adapter'):\n",
        "                    adapted_state = self.state_adapter(state)\n",
        "                    decoder_states.append(adapted_state)\n",
        "                else:\n",
        "                    decoder_states.append(state)\n",
        "\n",
        "        return decoder_states\n",
        "\n",
        "    def decode_step(self, x, states,training=None):\n",
        "        \"\"\"\n",
        "        Perform one decoding step.\n",
        "\n",
        "        Args:\n",
        "            x: Input character tensor of shape (batch_size, 1)\n",
        "            states: Previous states from the decoder\n",
        "\n",
        "        Returns:\n",
        "            output: Output probabilities\n",
        "            new_states: Updated states\n",
        "        \"\"\"\n",
        "        # Embed input character\n",
        "        x = self.target_embedding(x)\n",
        "\n",
        "        # Process through decoder layers\n",
        "        all_new_states = []\n",
        "        layer_output = x\n",
        "\n",
        "        # Track which state to use for each RNN layer\n",
        "        state_idx = 0\n",
        "        layer_idx = 0\n",
        "\n",
        "        for layer in self.decoder_layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                layer_output = layer(layer_output,training=training)\n",
        "            else:\n",
        "                # Extract the states for this layer\n",
        "                if self.cell_type == 'lstm':\n",
        "                    layer_states = [states[state_idx], states[state_idx+1]]\n",
        "                    state_idx += 2\n",
        "                else:  # RNN or GRU\n",
        "                    layer_states = [states[state_idx]]\n",
        "                    state_idx += 1\n",
        "\n",
        "                # Process through the layer\n",
        "                if self.cell_type == 'lstm':\n",
        "                    layer_output, state_h, state_c = layer(layer_output, initial_state=layer_states)\n",
        "                    all_new_states.extend([state_h, state_c])\n",
        "                else:  # RNN or GRU\n",
        "                    layer_output, state = layer(layer_output, initial_state=layer_states)\n",
        "                    all_new_states.append(state)\n",
        "\n",
        "                layer_idx += 1\n",
        "\n",
        "        # Apply final dropout if specified\n",
        "        if hasattr(self, 'final_dropout'):\n",
        "            layer_output = self.final_dropout(layer_output)\n",
        "\n",
        "        # Generate output probabilities\n",
        "        output = self.fc(layer_output)\n",
        "\n",
        "        return output, all_new_states\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            inputs: Tuple of (input_seq, target_seq)\n",
        "            training: Whether the model is in training mode\n",
        "\n",
        "        Returns:\n",
        "            outputs: Sequence of output probabilities\n",
        "        \"\"\"\n",
        "        input_seq, target_seq = inputs\n",
        "\n",
        "        # Encode the input sequence\n",
        "        encoder_outputs, encoder_states = self.encode(input_seq)\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize decoder states with encoder states\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Teacher forcing: feeding the target as the next input\n",
        "        decoder_inputs = target_seq[:, :-1]  # exclude last character\n",
        "\n",
        "        # Initialize list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        # Process each character in the target sequence\n",
        "        for t in range(decoder_inputs.shape[1]):\n",
        "            # Extract input for this timestep\n",
        "            decoder_input = decoder_inputs[:, t:t+1]\n",
        "\n",
        "            # Perform one decoding step\n",
        "            output, decoder_states = self.decode_step(decoder_input, decoder_states,training=training)\n",
        "\n",
        "            # Store output\n",
        "            outputs.append(output)\n",
        "        output, decoder_states = self.decode_step(decoder_input, decoder_states, training=training)\n",
        "        # Concatenate outputs along time axis\n",
        "        return tf.concat(outputs, axis=1)\n",
        "\n",
        "    def beam_search_decode(self, input_seq, max_length, beam_size=3, start_token=None, end_token=None):\n",
        "        \"\"\"\n",
        "        Generate a transliteration using beam search.\n",
        "\n",
        "        Args:\n",
        "            input_seq: Input sequence tensor of shape (batch_size, seq_length)\n",
        "            max_length: Maximum length of the output sequence\n",
        "            beam_size: Number of beams to track\n",
        "            start_token: ID of the start token\n",
        "            end_token: ID of the end token\n",
        "\n",
        "        Returns:\n",
        "            best_sequences: List of best sequences found (one per input sequence)\n",
        "        \"\"\"\n",
        "        batch_size = input_seq.shape[0]\n",
        "\n",
        "        # Default tokens if not provided\n",
        "        if start_token is None:\n",
        "            start_token = self.target_vocab_size - 2\n",
        "        if end_token is None:\n",
        "            end_token = self.target_vocab_size - 1\n",
        "\n",
        "        # Encode input sequence\n",
        "        _, encoder_states = self.encode(input_seq)\n",
        "\n",
        "        # Process each input sequence separately (beam search isn't batch-friendly)\n",
        "        best_sequences = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # Extract states for this batch item\n",
        "            if self.cell_type == 'lstm':\n",
        "                batch_states = []\n",
        "                for i in range(0, len(encoder_states), 2):\n",
        "                    batch_states.append(encoder_states[i][b:b+1])\n",
        "                    batch_states.append(encoder_states[i+1][b:b+1])\n",
        "            else:\n",
        "                batch_states = [state[b:b+1] for state in encoder_states]\n",
        "\n",
        "            # Initialize beam with start token\n",
        "            beams = [(0.0, [start_token], batch_states)]\n",
        "            complete_beams = []\n",
        "\n",
        "            # Generate sequence\n",
        "            for t in range(max_length):\n",
        "                new_beams = []\n",
        "\n",
        "                # Process each beam\n",
        "                for score, sequence, states in beams:\n",
        "                    # Check if sequence is complete\n",
        "                    if sequence[-1] == end_token:\n",
        "                        complete_beams.append((score, sequence))\n",
        "                        continue\n",
        "\n",
        "                    # Get next token predictions\n",
        "                    decoder_input = tf.constant([[sequence[-1]]], dtype=tf.int32)\n",
        "                    output, new_states = self.decode_step(decoder_input, states)\n",
        "\n",
        "                    # Get top k predictions\n",
        "                    log_probs = tf.math.log(output[0, 0])\n",
        "                    top_k_probs, top_k_indices = tf.math.top_k(log_probs, k=beam_size)\n",
        "\n",
        "                    # Create new beams\n",
        "                    for i in range(beam_size):\n",
        "                        new_score = score + top_k_probs[i].numpy()\n",
        "                        new_sequence = sequence + [top_k_indices[i].numpy()]\n",
        "                        new_beams.append((new_score, new_sequence, new_states))\n",
        "\n",
        "                # Keep only the best beams\n",
        "                beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_size]\n",
        "\n",
        "                # Early stopping if all beams are complete\n",
        "                if all(beam[1][-1] == end_token for beam in beams):\n",
        "                    complete_beams.extend(beams)\n",
        "                    break\n",
        "\n",
        "            # Add incomplete beams to complete ones\n",
        "            complete_beams.extend([(score, sequence) for score, sequence, _ in beams if sequence[-1] != end_token])\n",
        "\n",
        "            # Sort and select the best sequence\n",
        "            if complete_beams:\n",
        "                best_sequence = sorted(complete_beams, key=lambda x: x[0], reverse=True)[0][1]\n",
        "                # Remove start and end tokens\n",
        "                if best_sequence[-1] == end_token:\n",
        "                    best_sequence = best_sequence[1:-1]\n",
        "                else:\n",
        "                    best_sequence = best_sequence[1:]\n",
        "            else:\n",
        "                best_sequence = []\n",
        "\n",
        "            best_sequences.append(best_sequence)\n",
        "\n",
        "        return best_sequences\n",
        "\n",
        "# Decoding and evaluation functions\n",
        "def decode_sequences(model, sequences, start_token, end_token, max_length, beam_size=1):\n",
        "    \"\"\"\n",
        "    Decode sequences using the model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        sequences: Input sequences to decode\n",
        "        start_token, end_token: Tokens to mark sequence start/end\n",
        "        max_length: Maximum output sequence length\n",
        "        beam_size: Beam size for beam search (1 = greedy)\n",
        "\n",
        "    Returns:\n",
        "        predicted_sequences: List of predicted token sequences\n",
        "    \"\"\"\n",
        "    if beam_size > 1:\n",
        "        return model.beam_search_decode(\n",
        "            sequences,\n",
        "            max_length=max_length,\n",
        "            beam_size=beam_size,\n",
        "            start_token=start_token,\n",
        "            end_token=end_token\n",
        "        )\n",
        "    else:\n",
        "        # Initialize with start token\n",
        "        batch_size = sequences.shape[0]\n",
        "        current_tokens = tf.ones((batch_size, 1), dtype=tf.int32) * start_token\n",
        "        predicted_sequences = []\n",
        "\n",
        "        # Encode input\n",
        "        _, encoder_states = model.encode(sequences)\n",
        "        decoder_states = encoder_states\n",
        "\n",
        "        # Generate tokens one by one\n",
        "        for t in range(max_length):\n",
        "            # Predict next token\n",
        "            output, decoder_states = model.decode_step(current_tokens, decoder_states)\n",
        "            predicted_token = tf.argmax(output, axis=-1)\n",
        "\n",
        "            # Store prediction\n",
        "            predicted_sequences.append(predicted_token)\n",
        "\n",
        "            # Check for end tokens\n",
        "            if tf.reduce_all(predicted_token == end_token):\n",
        "                break\n",
        "\n",
        "            # Update current token\n",
        "            current_tokens = predicted_token\n",
        "\n",
        "        # Concatenate and convert to list\n",
        "        predicted_sequences = tf.concat(predicted_sequences, axis=1).numpy()\n",
        "\n",
        "        # Remove end tokens and convert to list\n",
        "        result = []\n",
        "        for seq in predicted_sequences:\n",
        "            # Find position of end token\n",
        "            try:\n",
        "                end_pos = list(seq).index(end_token)\n",
        "                result.append(list(seq[:end_pos]))\n",
        "            except ValueError:\n",
        "                # No end token found\n",
        "                result.append(list(seq))\n",
        "\n",
        "        return result\n",
        "\n",
        "def calculate_accuracy(true_seqs, pred_seqs):\n",
        "    \"\"\"\n",
        "    Calculate character and word accuracy between true and predicted sequences.\n",
        "\n",
        "    Args:\n",
        "        true_seqs: List of true sequences\n",
        "        pred_seqs: List of predicted sequences\n",
        "\n",
        "    Returns:\n",
        "        Dict with character and word level accuracy\n",
        "    \"\"\"\n",
        "    char_correct = 0\n",
        "    char_total = 0\n",
        "    word_correct = 0\n",
        "    word_total = len(true_seqs)\n",
        "\n",
        "    for true_seq, pred_seq in zip(true_seqs, pred_seqs):\n",
        "        # Word is correct if all characters match\n",
        "        word_is_correct = True\n",
        "\n",
        "        # Count character matches\n",
        "        for i in range(min(len(true_seq), len(pred_seq))):\n",
        "            if true_seq[i] == pred_seq[i]:\n",
        "                char_correct += 1\n",
        "            else:\n",
        "                word_is_correct = False\n",
        "\n",
        "        # Add extra characters as errors\n",
        "        if len(true_seq) != len(pred_seq):\n",
        "            word_is_correct = False\n",
        "\n",
        "        # Count total characters\n",
        "        char_total += max(len(true_seq), len(pred_seq))\n",
        "\n",
        "        # Update word accuracy\n",
        "        if word_is_correct:\n",
        "            word_correct += 1\n",
        "\n",
        "    return {\n",
        "        'char_accuracy': char_correct / char_total if char_total > 0 else 0,\n",
        "        'word_accuracy': word_correct / word_total if word_total > 0 else 0\n",
        "    }\n",
        "\n",
        "def tokens_to_text(sequences, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert token sequences back to text.\n",
        "\n",
        "    Args:\n",
        "        sequences: List of token sequences\n",
        "        tokenizer: Keras tokenizer\n",
        "\n",
        "    Returns:\n",
        "        List of text sequences\n",
        "    \"\"\"\n",
        "    # Reverse the word index\n",
        "    index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "    # Convert sequences to text\n",
        "    texts = []\n",
        "    for seq in sequences:\n",
        "        text = ''.join(index_word.get(token, '') for token in seq if token in index_word)\n",
        "        texts.append(text)\n",
        "\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "hzLv-_I-wQ4-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HD_ONk_wqFNr",
        "outputId": "fe2d26fe-6d47-49f4-e426-05fb3c3ef116"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: tmns1gtv\n",
            "Sweep URL: https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6d0k6xjx with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_units: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_decoder_units_pair: {'decoder_units': 256, 'encoder_units': 256}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_units: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout_rate: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmfazal735\u001b[0m (\u001b[33mmfazal735-iit-madras-foundation\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250517_140224-6d0k6xjx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/6d0k6xjx' target=\"_blank\">rich-sweep-1</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/6d0k6xjx' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/6d0k6xjx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 106260 training, 10424 dev, and 10517 test examples.\n",
            "  native latin  n\n",
            "0     آؤ   aao  3\n",
            "1     آؤ  aaoo  1\n",
            "2     آؤ   aau  1\n",
            "3     آؤ   aaw  1\n",
            "4     آؤ  aawo  1\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'transliteration_seq2_seq' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Exception encountered when calling LSTMCell.call().\n",
            "\n",
            "\u001b[1mDimensions must be equal, but are 32 and 256 for '{{node decoder_layer_1_1/lstm_cell_1/MatMul_4}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](encoder_layer_1_1/while:4, decoder_layer_1_1/lstm_cell_1/strided_slice)' with input shapes: [?,32], [256,256].\u001b[0m\n",
            "\n",
            "Arguments received by LSTMCell.call():\n",
            "  • inputs=tf.Tensor(shape=(None, 16), dtype=float32)\n",
            "  • states=('tf.Tensor(shape=(None, 32), dtype=float32)', 'tf.Tensor(shape=(None, 32), dtype=float32)')\n",
            "  • training=False''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transliteration_seq2_seq', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-sweep-1</strong> at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/6d0k6xjx' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/6d0k6xjx</a><br> View project at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250517_140224-6d0k6xjx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 6d0k6xjx errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-5-e0d7fb6fc152>\", line 154, in train_sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     history = model.fit(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-bf336aa5563b>\", line 236, in call\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, decoder_states = self.decode_step(decoder_input, decoder_states,training=training)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-bf336aa5563b>\", line 186, in decode_step\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     layer_output, state_h, state_c = layer(layer_output, initial_state=layer_states)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 144, in capture_by_value\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     graph._validate_in_scope(tensor)  # pylint: disable=protected-access\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m tensorflow.python.framework.errors_impl.InaccessibleTensorError: Exception encountered when calling LSTMCell.call().\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \u001b[1m<tf.Tensor 'decoder_layer_1_1/stateless_dropout/SelectV2:0' shape=(None, 32) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m <tf.Tensor 'decoder_layer_1_1/stateless_dropout/SelectV2:0' shape=(None, 32) dtype=float32> was defined here:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/lib/python3.11/threading.py\", line 1002, in _bootstrap\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"<ipython-input-5-e0d7fb6fc152>\", line 154, in train_sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 57, in train_step\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 829, in __call__\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 1380, in _maybe_build\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\", line 224, in compute_output_spec\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"<ipython-input-4-bf336aa5563b>\", line 236, in call\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"<ipython-input-4-bf336aa5563b>\", line 186, in decode_step\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\", line 584, in call\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py\", line 398, in call\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py\", line 438, in _maybe_config_dropout_masks\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/dropout_rnn_cell.py\", line 50, in get_recurrent_dropout_mask\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/dropout_rnn_cell.py\", line 30, in _create_dropout_mask\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/dropout_rnn_cell.py\", line 31, in <listcomp>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/random.py\", line 88, in dropout\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The tensor <tf.Tensor 'decoder_layer_1_1/stateless_dropout/SelectV2:0' shape=(None, 32) dtype=float32> cannot be accessed from FuncGraph(name=one_step_on_data, id=136440805012672), because it was defined in FuncGraph(name=scratch_graph, id=136440805774656), which is out of scope.\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Arguments received by LSTMCell.call():\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • inputs=tf.Tensor(shape=(None, 16), dtype=float32)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • states=('tf.Tensor(shape=(None, 32), dtype=float32)', 'tf.Tensor(shape=(None, 32), dtype=float32)')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • training=True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5y2icm5g with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: gru\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_units: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_decoder_units_pair: {'decoder_units': 128, 'encoder_units': 128}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_units: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout_rate: 0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250517_140234-5y2icm5g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/5y2icm5g' target=\"_blank\">spring-sweep-2</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/5y2icm5g' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/5y2icm5g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 106260 training, 10424 dev, and 10517 test examples.\n",
            "  native latin  n\n",
            "0     آؤ   aao  3\n",
            "1     آؤ  aaoo  1\n",
            "2     آؤ   aau  1\n",
            "3     آؤ   aaw  1\n",
            "4     آؤ  aawo  1\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'transliteration_seq2_seq' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''list index out of range''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transliteration_seq2_seq', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">spring-sweep-2</strong> at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/5y2icm5g' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/5y2icm5g</a><br> View project at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250517_140234-5y2icm5g/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5y2icm5g errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-5-e0d7fb6fc152>\", line 154, in train_sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     history = model.fit(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-bf336aa5563b>\", line 236, in call\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, decoder_states = self.decode_step(decoder_input, decoder_states,training=training)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-bf336aa5563b>\", line 181, in decode_step\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     layer_states = [states[state_idx]]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ~~~~~~^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m IndexError: Exception encountered when calling TransliterationSeq2Seq.call().\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \u001b[1mlist index out of range\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Arguments received by TransliterationSeq2Seq.call():\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • inputs=('tf.Tensor(shape=(None, 23), dtype=int32)', 'tf.Tensor(shape=(None, 16), dtype=int32)')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • training=True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ib5rkb3b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_units: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_decoder_units_pair: {'decoder_units': 64, 'encoder_units': 64}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_units: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout_rate: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250517_140244-ib5rkb3b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ib5rkb3b' target=\"_blank\">visionary-sweep-3</a></strong> to <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/sweeps/tmns1gtv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ib5rkb3b' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ib5rkb3b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 106260 training, 10424 dev, and 10517 test examples.\n",
            "  native latin  n\n",
            "0     آؤ   aao  3\n",
            "1     آؤ  aaoo  1\n",
            "2     آؤ   aau  1\n",
            "3     آؤ   aaw  1\n",
            "4     آؤ  aawo  1\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'transliteration_seq2_seq' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Exception encountered when calling SimpleRNNCell.call().\n",
            "\n",
            "\u001b[1mDimensions must be equal, but are 64 and 32 for '{{node decoder_layer_1_1/simple_rnn_cell_1/MatMul_1}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](encoder_layer_1_1/while:4, decoder_layer_1_1/simple_rnn_cell_1/Cast_1/mul_1)' with input shapes: [?,64], [32,32].\u001b[0m\n",
            "\n",
            "Arguments received by SimpleRNNCell.call():\n",
            "  • sequence=tf.Tensor(shape=(None, 64), dtype=float32)\n",
            "  • states=('tf.Tensor(shape=(None, 64), dtype=float32)',)\n",
            "  • training=False''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transliteration_seq2_seq', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">visionary-sweep-3</strong> at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ib5rkb3b' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration/runs/ib5rkb3b</a><br> View project at: <a href='https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration' target=\"_blank\">https://wandb.ai/mfazal735-iit-madras-foundation/dakshina-transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250517_140244-ib5rkb3b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ib5rkb3b errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-5-e0d7fb6fc152>\", line 154, in train_sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     history = model.fit(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-bf336aa5563b>\", line 236, in call\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, decoder_states = self.decode_step(decoder_input, decoder_states,training=training)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-4-bf336aa5563b>\", line 189, in decode_step\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     layer_output, state = layer(layer_output, initial_state=layer_states)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m ValueError: Exception encountered when calling SimpleRNNCell.call().\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \u001b[1mDimensions must be equal, but are 64 and 32 for '{{node transliteration_seq2_seq_1/decoder_layer_1_1/simple_rnn_cell_1/MatMul_1}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](transliteration_seq2_seq_1/encoder_layer_1_1/while:4, transliteration_seq2_seq_1/decoder_layer_1_1/simple_rnn_cell_1/Cast_1/ReadVariableOp)' with input shapes: [?,64], [32,32].\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Arguments received by SimpleRNNCell.call():\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • sequence=tf.Tensor(shape=(None, 64), dtype=float32)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • states=('tf.Tensor(shape=(None, 64), dtype=float32)',)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   • training=True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
          ]
        }
      ],
      "source": [
        "# Model creation with dropout support\n",
        "def create_model(input_vocab_size,\n",
        "                target_vocab_size,\n",
        "                embedding_dim,\n",
        "                encoder_units,\n",
        "                decoder_units,\n",
        "                cell_type='lstm',\n",
        "                encoder_layers=1,\n",
        "                decoder_layers=1,\n",
        "                dropout_rate=0.0,\n",
        "                recurrent_dropout_rate=0.0):\n",
        "    \"\"\"\n",
        "    Create and compile a transliteration model.\n",
        "\n",
        "    Args:\n",
        "        input_vocab_size: Size of the input vocabulary\n",
        "        target_vocab_size: Size of the target vocabulary\n",
        "        embedding_dim: Dimension of character embeddings\n",
        "        encoder_units: Number of units in encoder cell\n",
        "        decoder_units: Number of units in decoder cell\n",
        "        cell_type: Type of RNN cell ('rnn', 'lstm', or 'gru')\n",
        "        encoder_layers: Number of layers in the encoder\n",
        "        decoder_layers: Number of layers in the decoder\n",
        "        dropout_rate: Dropout rate after RNN layers\n",
        "        recurrent_dropout_rate: Dropout rate inside RNN cells\n",
        "\n",
        "    Returns:\n",
        "        model: Compiled transliteration model\n",
        "    \"\"\"\n",
        "    # Create inputs\n",
        "    input_seq = Input(shape=(None,), name='input_sequence')\n",
        "    target_seq = Input(shape=(None,), name='target_sequence')\n",
        "\n",
        "    # Create model\n",
        "    model = TransliterationSeq2Seq(\n",
        "        input_vocab_size=input_vocab_size,\n",
        "        target_vocab_size=target_vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        encoder_units=encoder_units,\n",
        "        decoder_units=decoder_units,\n",
        "        cell_type=cell_type,\n",
        "        encoder_layers=encoder_layers,\n",
        "        decoder_layers=decoder_layers,\n",
        "        dropout_rate=dropout_rate,\n",
        "        recurrent_dropout_rate=recurrent_dropout_rate\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# WandB setup and hyperparameter sweep\n",
        "def setup_wandb_sweep():\n",
        "    \"\"\"\n",
        "    Setup wandb hyperparameter sweep.\n",
        "\n",
        "    Returns:\n",
        "        sweep_id: ID of the created sweep\n",
        "    \"\"\"\n",
        "    sweep_config = {\n",
        "        'method': 'bayes',  # Bayesian optimization\n",
        "        'metric': {\n",
        "            'name': 'val_accuracy',\n",
        "            'goal': 'maximize'\n",
        "        },\n",
        "        'parameters': {\n",
        "            'embedding_dim': {\n",
        "                'values': [16, 32, 64]\n",
        "            },\n",
        "            'encoder_units': {\n",
        "                'values': [32, 64, 128, 256]\n",
        "            },\n",
        "            'decoder_units': {\n",
        "                'values': [32, 64, 128, 256]\n",
        "            },\n",
        "\n",
        "            \"encoder_decoder_units_pair\": {\n",
        "            \"values\": [\n",
        "                {\"encoder_units\": 32, \"decoder_units\": 32},\n",
        "                {\"encoder_units\": 64, \"decoder_units\": 64},\n",
        "                {\"encoder_units\": 128, \"decoder_units\": 128},\n",
        "                {\"encoder_units\": 256, \"decoder_units\": 256}\n",
        "            ]},\n",
        "            'encoder_layers': {\n",
        "                'values': [1, 2, 3]\n",
        "            },\n",
        "            'decoder_layers': {\n",
        "                'values': [1, 2, 3]\n",
        "            },\n",
        "            'cell_type': {\n",
        "                'values': ['lstm', 'gru', 'rnn']\n",
        "            },\n",
        "            'dropout_rate': {\n",
        "                'values': [0.0, 0.2, 0.3]\n",
        "            },\n",
        "            'recurrent_dropout_rate': {\n",
        "                'values': [0.0, 0.1, 0.2]\n",
        "            },\n",
        "            'beam_size': {\n",
        "                'values': [1, 3, 5]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create sweep\n",
        "    sweep_id = wandb.sweep(sweep_config, project=\"dakshina-transliteration\")\n",
        "\n",
        "    return sweep_id\n",
        "\n",
        "\n",
        "# Training function for wandb sweep\n",
        "def train_sweep():\n",
        "    \"\"\"\n",
        "    Training function for wandb sweep.\n",
        "    \"\"\"\n",
        "    # Initialize wandb\n",
        "    run = wandb.init()\n",
        "\n",
        "    # Get wandb config\n",
        "    config = wandb.config\n",
        "\n",
        "    # Load data\n",
        "    train_data, dev_data, test_data = load_dakshina_data()\n",
        "    preprocessed_data = preprocess_data(train_data, dev_data, test_data)\n",
        "\n",
        "    # Create model with config parameters\n",
        "    model = create_model(\n",
        "        input_vocab_size=preprocessed_data['latin_vocab_size'],\n",
        "        target_vocab_size=preprocessed_data['native_vocab_size'],\n",
        "        embedding_dim=config.embedding_dim,\n",
        "        encoder_units=config.encoder_units,\n",
        "        decoder_units=config.decoder_units,\n",
        "        cell_type=config.cell_type,\n",
        "        encoder_layers=config.encoder_layers,\n",
        "        decoder_layers=config.decoder_layers,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "        recurrent_dropout_rate=config.recurrent_dropout_rate\n",
        "    )\n",
        "\n",
        "    # # Setup callbacks\n",
        "    # callbacks = [\n",
        "    #     WandbCallback(),\n",
        "    #     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    #     ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "    # ]\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        [preprocessed_data['train_latin'], preprocessed_data['train_native']],\n",
        "        preprocessed_data['train_native_target'],\n",
        "        validation_data=(\n",
        "            [preprocessed_data['dev_latin'], preprocessed_data['dev_native']],\n",
        "            preprocessed_data['dev_native_target']\n",
        "        ),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        # callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Evaluate with beam search\n",
        "    beam_size = config.beam_size\n",
        "\n",
        "    # Decode dev set with beam search\n",
        "    pred_dev_sequences = decode_sequences(\n",
        "        model,\n",
        "        preprocessed_data['dev_latin'],\n",
        "        preprocessed_data['start_token'],\n",
        "        preprocessed_data['end_token'],\n",
        "        preprocessed_data['max_native_len'],\n",
        "        beam_size=beam_size\n",
        "    )\n",
        "\n",
        "    # Remove start/end tokens from the target sequences\n",
        "    true_dev_sequences = [seq[1:-1] for seq in preprocessed_data['dev_native'].tolist()]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = calculate_accuracy(true_dev_sequences, pred_dev_sequences)\n",
        "\n",
        "    # Log metrics\n",
        "    wandb.log({\n",
        "        'dev_char_accuracy': accuracy['char_accuracy'],\n",
        "        'dev_word_accuracy': accuracy['word_accuracy']\n",
        "    })\n",
        "\n",
        "    # Convert some examples to text for visual inspection\n",
        "    native_tokenizer = preprocessed_data['native_tokenizer']\n",
        "    latin_tokenizer = preprocessed_data['latin_tokenizer']\n",
        "\n",
        "    # Get sample predictions\n",
        "    sample_idx = np.random.choice(len(pred_dev_sequences), min(10, len(pred_dev_sequences)), replace=False)\n",
        "    samples = []\n",
        "\n",
        "    for idx in sample_idx:\n",
        "        input_text = tokens_to_text([preprocessed_data['dev_latin'][idx].tolist()], latin_tokenizer)[0]\n",
        "        true_text = tokens_to_text([true_dev_sequences[idx]], native_tokenizer)[0]\n",
        "        pred_text = tokens_to_text([pred_dev_sequences[idx]], native_tokenizer)[0]\n",
        "\n",
        "        samples.append({\n",
        "            'input': input_text,\n",
        "            'true': true_text,\n",
        "            'pred': pred_text\n",
        "        })\n",
        "\n",
        "    # Log sample predictions\n",
        "    wandb.log({'samples': wandb.Table(\n",
        "        columns=['Input', 'True', 'Predicted'],\n",
        "        data=[[s['input'], s['true'], s['pred']] for s in samples]\n",
        "    )})\n",
        "\n",
        "    # Clean up\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "# Main script for running the sweep\n",
        "def main():\n",
        "    \"\"\"Main function to run the sweep.\"\"\"\n",
        "    sweep_id = setup_wandb_sweep()\n",
        "    wandb.agent(sweep_id, train_sweep, count=100)  # Run 20 experiments\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   List item\n",
        "\n",
        "*   List item\n",
        "\n",
        "*   List item\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "*   List item\n",
        "\n",
        "\n",
        "*   List item\n",
        "\n",
        "\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "e4d0a8c3ccaf2534e9ab91c659e420ba5114533f"
      ],
      "metadata": {
        "id": "MV5oW4cxJHDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eSBKhHWurtqH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPsLp6Pwe868PLKZKRCJezx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}